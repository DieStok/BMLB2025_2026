{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 6\n",
    "\n",
    "Here you'll be burying the traumatic experience of training your own neural networks by doing it _much_ more easily using Keras. We'll work on the same MNIST dataset as before. First run the two code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "import itertools\n",
    "\n",
    "#Keras import\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import torch\n",
    "#if Mac Apple silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    #fix for MPS \n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.wrappers import SKLearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7dafdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data. Liberally copied from: https://keras.io/examples/vision/mnist_convnet/\n",
    "\n",
    "# # Model / data parameters\n",
    "# num_classes = 10\n",
    "# input_shape = (28, 28, 1)\n",
    "\n",
    "# # the data, split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# # Scale images to the [0, 1] range\n",
    "# x_train = x_train.astype(\"float32\") / 255\n",
    "# x_test = x_test.astype(\"float32\") / 255\n",
    "# # Make sure images have shape (28, 28, 1)\n",
    "# x_train = np.expand_dims(x_train, -1)\n",
    "# x_test = np.expand_dims(x_test, -1)\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(x_train.shape[0], \"train samples\")\n",
    "# print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11f405",
   "metadata": {},
   "source": [
    "## Getting a grip on the Keras functional API\n",
    "\n",
    "We want to remake our fully-connected neural network that we made last week in Keras as a first step. So, let's make sure you understand the Keras functional API. To do so, follow along with [this link](https://keras.io/guides/functional_api/) up to and until Training, evaluation, and inference.\n",
    "\n",
    "**NOTE: TO BE ABLE TO SHOW THE DIAGRAM, YOU NEED TO RUN conda install pydot IN CONDA (IN YOUR COURSE ENVIRONMENT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a011c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mnist_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mnist_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Downsampled x_train shape: (20000, 784)\n",
      "Downsampled x_test shape: (1000, 784)\n",
      "Got here 1\n",
      "Got here 2\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# diagnostic\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot here 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_scores[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# your code from following along here. Or if you want it spread over cells, add new ones with esc+b\n",
    "inputs = keras.Input(shape=(784,))\n",
    "print(inputs.shape)\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "print(model.summary())\n",
    "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "train_indices = rng.choice(x_train.shape[0], 20000, replace=False)\n",
    "test_indices  = rng.choice(x_test.shape[0], 1000, replace=False)\n",
    "\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]  # Downsample labels as well.\n",
    "x_test  = x_test[test_indices]\n",
    "y_test  = y_test[test_indices]\n",
    "\n",
    "print(\"Downsampled x_train shape:\", x_train.shape)\n",
    "print(\"Downsampled x_test shape:\", x_test.shape)\n",
    "\n",
    "print('Got here 1')\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# diagnostic\n",
    "print('Got here 2')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7201868",
   "metadata": {},
   "source": [
    "## Remaking our model from last week\n",
    "\n",
    "You now basically have all the ingredients to remake your model. In fact, you already made a larger model above. Now, try to do the same thing for the neural network architecture that we used last week, which looked like this:\n",
    "![NeuralNetworkImage](NeuralNetwork.PNG)\n",
    "We will use one Hidden Layer with 25 units, and one output layer with 10 units. Use the sigmoid activation that we've used before. See [here](https://keras.io/api/layers/activations/) if you are lost on how to use the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58852652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# answer\n",
    "#%pdb on\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "print('Got here 1')\n",
    "\n",
    "\n",
    "ourInputs = keras.Input(shape=(784,))\n",
    "\n",
    "x = layers.Dense(25, activation=\"sigmoid\")(ourInputs)\n",
    "ourOutputs = layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "ourModel = keras.Model(inputs=ourInputs, outputs=ourOutputs, name=\"mnist_model\")\n",
    "\n",
    "ourModel.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = ourModel.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "test_scores = ourModel.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "keras.utils.plot_model(ourModel, \"lastWeekModel.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a0dd",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "For one layer and about 80 neurons less you lose 5 percentage points of accuracy. Our simple network is still not too bad. \n",
    "\n",
    "## Using the softmax function and ReLUs\n",
    "\n",
    "Until now, we've been allowing the outputs of our network to sum up to more than one. That's fine for predictions, since we just pick the node with the highest activation and take that as our predicted class. Still, it might be nice, at least for interpretability, if we compress the sum of all these outputs to 1. Then we can interpret each output as a probability of being that class.\n",
    "\n",
    "What's more, sigmoid neurons have some problems, and Rectified Linear Units (linear, except for negative values which become 0) have been shown to work very well in deep neural networks. Note that they are nonlinear for data < 0, and otherwise linear. That's why they still count as nonlinear activation functions. See [this](https://stats.stackexchange.com/a/510818) or [this](https://www.youtube.com/watch?v=68BZ5f7P94E) for more info (**optional**).\n",
    "\n",
    "Over to you: use the same network you just trained, only switch to ReLUs and use a softmax output layer! Also, train for 4 instead of 2 epochs (4 full passes over the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# answer\n",
    "ourInputs = keras.Input(shape=(784,))\n",
    "\n",
    "x = layers.Dense(25, activation=\"relu\")(ourInputs)\n",
    "ourOutputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "ourModel = keras.Model(inputs=ourInputs, outputs=ourOutputs, name=\"mnist_model\")\n",
    "\n",
    "ourModel.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = ourModel.fit(x_train, y_train, batch_size=64, epochs=4, validation_split=0.2)\n",
    "\n",
    "test_scores = ourModel.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "keras.utils.plot_model(ourModel, \"lastWeekModel.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f8178",
   "metadata": {},
   "source": [
    "## What you see\n",
    "\n",
    "If all goes well, you should see you get a slightly better outcome in this way. This is only due to the ReLUs and/or a different initialisation. I ran it 3 times, and got a worse performance once and a better performance twice. Remember, neural networks get random initialisations to break the symmetry that otherwise occurs in the errors of neurons (causing everything to update the exact same way).\n",
    "\n",
    "## Training a convolutional neural network\n",
    "Using Keras we are not beholden to simple dense networks. We can quite easily train a convolutional neural network. Let's do that now and see its performance. Go [here](https://keras.io/examples/vision/mnist_convnet/) and follow the instructions. **Make sure you understand what each layer does, so quickly look up Dropout and Flatten to see what they might do!**. Note that this tutorial doesn't use the functional API but the `Sequential()` syntax. It leads to the same thing, and it is good for you to see that these two different ways of defining Keras models exist!\n",
    "\n",
    "**Beware: training this network for 15 epochs takes quite some time. Reduce it to 8 epochs instead of the 15 in the example for speed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n",
    "#answer\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "modelConv = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "modelConv.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15 #change to something like 5 or 7 for speed in your code!\n",
    "\n",
    "modelConv.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "modelConv.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = modelConv.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a40c3",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "You should see that there is indeed an accuracy of ~99% on the test set. For fewer epochs it might be more ~98%, but still. _Pretty nifty_ for a few lines of code. \n",
    "\n",
    "Do note that, if you were to run this code on real handwritten digits, you still need to solve the problem of segmenting out the digits in an image. Also note that in this dataset all the ones look like | and not so much like 1. So a 1 might register as a 7. The point is that this classifier works fine for the narrow task you gave it, so it will perform nigh-on perfectly on classifying MNIST digits, but add a little noise or a small difference, and there's still a problem. It generalises well to the _specific_ problem of MNIST data, but certainly not _as well_ to the actual problem of digit recognition. It is still just a toy classifier. See [here](https://matteo-a-barbieri.medium.com/why-mnist-is-the-worst-thing-that-has-ever-happened-to-humanity-49fd053f0f66) for more info.\n",
    "\n",
    "## Finally: integrating neural networks with scikit-learn for (nested) cross-validation\n",
    "\n",
    "What we _want_ to do is to be able to take a neural network and use the tools you have learned for cross-validation to improve it. Ideally, you'd use a scikit-learn Pipeline with a StandardScaler, and then perhaps a dimension reduction step if necessary, and then a neural network, all with tunable hyperparameters, which could all be tuned at once with the pipeline and GridSearchCV or RandomSearchCV. \n",
    "\n",
    "Luckily, this is possible. We just need to do two things:\n",
    "1. Make a _function_ that constructs a Keras neural network model, that takes arguments which are parameters we might want to change. Say, the learning rate, size of Hidden Layer 1, and size of Hidden Layer 2. \n",
    "2. Instantiate a scikit-learn model object built from our neural network, using `KerasClassifier`\n",
    "\n",
    "This is not too difficult. An example of building such a function and instantiating a model is shown below. Make sure you understand the steps: making a model-building function in Keras, wrapping that function in a call that lets it behave as a scikit-learn classifier, and then hyperparameter tuning as normal (here without nested cross-val for speed).\n",
    "\n",
    "**Note: RandomisedSearchCV depends on random numbers, so your outcome will be slightly different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2ed66-0e7b-489f-a772-bd5d75c6bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "randomNumberMaker = np.random.default_rng(seed=42)\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Subsample for faster runtime\n",
    "indicesTrain = randomNumberMaker.integers(0, len(x_train), 3000)\n",
    "indicesTest  = randomNumberMaker.integers(0, len(x_test), 200)\n",
    "x_train = x_train[indicesTrain]\n",
    "y_train = y_train[indicesTrain]\n",
    "x_test  = x_test[indicesTest]\n",
    "y_test  = y_test[indicesTest]\n",
    "\n",
    "def build_neural_net(hiddenLayerOne=784, hiddenLayerTwo=256, learnRate=0.01, **kwargs):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(Dense(hiddenLayerOne, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hiddenLayerTwo, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learnRate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create an instance of SKLearnClassifier.\n",
    "classifier = SKLearnClassifier(\n",
    "    model=build_neural_net,\n",
    "    warm_start=False,\n",
    "    model_kwargs={},             # Initially empty; hyperparameters come from the grid.\n",
    "    fit_kwargs={'verbose': 0}\n",
    ")\n",
    "\n",
    "# Construct a parameter grid with nested model_kwargs.\n",
    "param_grid = dict(\n",
    "    model_kwargs=[\n",
    "        {'hiddenLayerOne': hl1, 'hiddenLayerTwo': hl2, 'learnRate': lr}\n",
    "        for hl1 in [5, 10, 50]\n",
    "        for hl2 in [5, 10, 50]\n",
    "        for lr in [1e-2, 1e-3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_distributions=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    n_iter=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "searchResults = search.fit(x_train, y_train)\n",
    "bestScore = searchResults.best_score_\n",
    "bestParams = searchResults.best_params_\n",
    "print(\"Best score is {:.2f} using {}\".format(bestScore, bestParams))\n",
    "\n",
    "bestModel = searchResults.best_estimator_\n",
    "accuracy = bestModel.score(x_test, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ec8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c26adc3",
   "metadata": {},
   "source": [
    "## What I want you to remember here\n",
    "\n",
    "There is of course a lot more nuance and power in Keras, this is just the tip of the iceberg. But the practical skills you need to take from here:\n",
    "* You know how to build and train a neural network model using Keras (both Dense and Convolutional models).\n",
    "* You know how to wrap a Keras model in a constructor function, and instantiate it as a sklearn model, so that you can use it with sklearn for hyperparameter optimalisation\n",
    "\n",
    "## The end\n",
    "\n",
    "With that, the course content is done. All that's left is to apply ML to an example dataset using your understanding of the fundamentals of what's _really_ going on, and the high-level high-powered libraries that you can now command. \n",
    "\n",
    "## Survey\n",
    "[Click me, click me. Oh, oh, oh click me!](https://docs.google.com/forms/d/e/1FAIpQLScURtN2iRNJrLGHqYLxOOZ8Fz8xjoqsIcC_lGzfwuukRvZ0Ew/viewform?usp=sf_link) - after Donkey (on the Shrek DVD main menu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
