{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 6\n",
    "\n",
    "Here you'll be burying the traumatic experience of training your own neural networks by doing it _much_ more easily using Keras. We'll work on the same MNIST dataset as before. First run the two code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "import itertools\n",
    "##IF YOU HAVE TENSORFLOW\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "##I don't know why, but for some people these 2 extra imports are necessary for Keras to work\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "# make it so you can use exactly the code from the tutorial\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.wrappers import SKLearnClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11f405",
   "metadata": {},
   "source": [
    "## Note beforehand (2024):\n",
    "I asked you to install Keras with the PyTorch backend, but the wrapper for Keras seems to require TensorFlow. We'll see how it goes. Worst-case scenario you need a separate environment with tensorflow (see installation instructions here: https://docs.anaconda.com/free/working-with-conda/applications/tensorflow/ )\n",
    "\n",
    "## Getting a grip on the Keras functional API\n",
    "\n",
    "We want to remake our fully-connected neural network that we made last week in Keras as a first step. So, let's make sure you understand the Keras functional API. To do so, follow along with [this link](https://keras.io/guides/functional_api/) up to and until Training, evaluation, and inference.\n",
    "\n",
    "**NOTE: TO BE ABLE TO SHOW THE DIAGRAM, YOU NEED TO RUN `conda install pydot` IN CONDA (IN YOUR COURSE ENVIRONMENT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a011c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional:\n",
    "#!conda install pydot\n",
    "\n",
    "# your code from following along here. Or if you want it spread over cells, add new ones with esc+b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7201868",
   "metadata": {},
   "source": [
    "## Remaking our model from last week\n",
    "\n",
    "You now basically have all the ingredients to remake your model. In fact, you already made a larger model above. Now, try to do the same thing for the neural network architecture that we used last week, which looked like this:\n",
    "![NeuralNetworkImage](NeuralNetwork.PNG)\n",
    "We will use one Hidden Layer with 25 units, and one output layer with 10 units. Use the sigmoid activation that we've used before. See [here](https://keras.io/api/layers/activations/) if you are lost on how to use the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58852652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a0dd",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "For one layer and about 80 neurons less you lose 5 percentage points of accuracy. Our simple network is still not too bad. \n",
    "\n",
    "## Using the softmax function and ReLUs\n",
    "\n",
    "Until now, we've been allowing the outputs of our network to sum up to more than one. That's fine for predictions, since we just pick the node with the highest activation and take that as our predicted class. Still, it might be nice, at least for interpretability, if we compress the sum of all these outputs to 1. Then we can interpret each output as a probability of being that class.\n",
    "\n",
    "What's more, sigmoid neurons have some problems, and Rectified Linear Units (linear, except for negative values which become 0) have been shown to work very well in deep neural networks. Note that they are nonlinear for data < 0, and otherwise linear. That's why they still count as nonlinear activation functions. See [this](https://stats.stackexchange.com/a/510818) or [this](https://www.youtube.com/watch?v=68BZ5f7P94E) for more info (**optional**).\n",
    "\n",
    "Over to you: use the same network you just trained, only switch to ReLUs and use a softmax output layer! Also, train for 4 instead of 2 epochs (4 full passes over the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0780b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f8178",
   "metadata": {},
   "source": [
    "## What you see\n",
    "\n",
    "If all goes well, you should see you get a slightly better outcome in this way. This is only due to the ReLUs and/or a different initialisation. I ran it 3 times, and got a worse performance once and a better performance twice. Remember, neural networks get random initialisations to break the symmetry that otherwise occurs in the errors of neurons (causing everything to update the exact same way).\n",
    "\n",
    "## Training a convolutional neural network\n",
    "Using Keras we are not beholden to simple dense networks. We can quite easily train a convolutional neural network. Let's do that now and see its performance. Go [here](https://keras.io/examples/vision/mnist_convnet/) and follow the instructions. **Make sure you understand what each layer does, so quickly look up Dropout and Flatten to see what they might do!**. For a few different dropout explanations, see [here](https://towardsdatascience.com/5-perspectives-to-why-dropout-works-so-well-1c10617b8028). Note that this tutorial doesn't use the functional API but the `Sequential()` syntax. It leads to the same thing, and it is good for you to see that these two different ways of defining Keras models exist!\n",
    "\n",
    "**Beware: training this network for 15 epochs takes quite some time. Reduce it to 8 epochs instead of the 15 in the example for speed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459fa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a40c3",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "You should see that there is indeed an accuracy of ~99% on the test set. For fewer epochs it might be more ~98%, but still. _Pretty nifty_ for a few lines of code. \n",
    "\n",
    "Do note that, if you were to run this code on real handwritten digits, you still need to solve the problem of segmenting out the digits in an image. Also note that in this dataset all the ones look like | and not so much like 1. So a 1 might register as a 7. The point is that this classifier works fine for the narrow task you gave it, so it will perform nigh-on perfectly on classifying MNIST digits, but add a little noise or a small difference, and there's still a problem. It generalises well to the _specific_ problem of MNIST data, but certainly not _as well_ to the actual problem of digit recognition. It is still just a toy classifier. See [here](https://matteo-a-barbieri.medium.com/why-mnist-is-the-worst-thing-that-has-ever-happened-to-humanity-49fd053f0f66) for more info.\n",
    "\n",
    "## Finally: integrating neural networks with scikit-learn for (nested) cross-validation\n",
    "\n",
    "What we _want_ to do is to be able to take a neural network and use the tools you have learned for cross-validation to improve it. Ideally, you'd use a scikit-learn Pipeline with a StandardScaler, and then perhaps a dimension reduction step if necessary, and then a neural network, all with tunable hyperparameters, which could all be tuned at once with the pipeline and GridSearchCV or RandomSearchCV. \n",
    "\n",
    "Luckily, this is possible. We just need to do two things:\n",
    "1. Make a _function_ that constructs a Keras neural network model, that takes arguments which are parameters we might want to change. Say, the learning rate, size of Hidden Layer 1, and size of Hidden Layer 2. \n",
    "2. Instantiate a scikit-learn model object built from our neural network, using `SKLearnClassifier` see [here](https://keras.io/api/utils/sklearn_wrappers/#sklearnclassifier-class)\n",
    "\n",
    "This is not too difficult. An example of building such a function and instantiating a model is shown below. Make sure you understand the steps: making a model-building function in Keras, wrapping that function in a call that lets it behave as a scikit-learn classifier, and then hyperparameter tuning as normal (here without nested cross-val for speed).\n",
    "\n",
    "**Note: RandomisedSearchCV depends on random numbers, so your outcome will be slightly different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17ec8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "3000 train samples after downsampling for faster runtime\n",
      "200 test samples after downsampling for faster runtime\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'learnRate' for estimator SKLearnClassifier(model=<function build_neural_net at 0x10fc1bdc0>,\n                  model_kwargs={}). Valid parameters are: ['fit_kwargs', 'model', 'model_kwargs', 'warm_start'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 854, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n  File \"/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/base.py\", line 283, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'learnRate' for estimator SKLearnClassifier(model=<function build_neural_net at 0x10fc1bdc0>,\n                  model_kwargs={}). Valid parameters are: ['fit_kwargs', 'model', 'model_kwargs', 'warm_start'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 94\u001b[0m\n\u001b[1;32m     84\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     85\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mclassifier,\n\u001b[1;32m     86\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Fit the model using randomized search.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m searchResults \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract and display the best score and hyperparameters.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m bestScore \u001b[38;5;241m=\u001b[39m searchResults\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1951\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'learnRate' for estimator SKLearnClassifier(model=<function build_neural_net at 0x10fc1bdc0>,\n                  model_kwargs={}). Valid parameters are: ['fit_kwargs', 'model', 'model_kwargs', 'warm_start']."
     ]
    }
   ],
   "source": [
    "from keras.wrappers import SKLearnClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV #rather than trying all combinations of hyperparams, sample some options from a range\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "randomNumberMaker = np.random.default_rng(seed=42)\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Ensure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# Subsample for faster runtime\n",
    "indicesTrain = randomNumberMaker.integers(0, len(x_train), 3000)\n",
    "indicesTest  = randomNumberMaker.integers(0, len(x_test), 200)\n",
    "x_train = x_train[indicesTrain]\n",
    "y_train = y_train[indicesTrain]\n",
    "x_test  = x_test[indicesTest]\n",
    "y_test  = y_test[indicesTest]\n",
    "print(x_train.shape[0], \"train samples after downsampling for faster runtime\")\n",
    "print(x_test.shape[0], \"test samples after downsampling for faster runtime\")\n",
    "\n",
    "# Function to build the neural network model.\n",
    "# Note: The function accepts **kwargs to satisfy the requirements of SKLearnClassifier.\n",
    "def build_neural_net(hiddenLayerOne=784, hiddenLayerTwo=256, learnRate=0.01, **kwargs):\n",
    "    model = Sequential()\n",
    "    # Flatten the 28x28 images into 784 pixel values.\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(Dense(hiddenLayerOne, activation=\"relu\"))\n",
    "    # Dropout can also be considered as a hyperparameter.\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hiddenLayerTwo, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # Softmax layer for multi-class classification.\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learnRate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create an instance of SKLearnClassifier using the new wrapper.\n",
    "classifier = SKLearnClassifier(\n",
    "    model=build_neural_net,      # A callable returning a compiled Keras model.\n",
    "    warm_start=False,            # Do not reuse weights from previous fits.\n",
    "    model_kwargs={},             # Additional keyword arguments for the model (if any).\n",
    "    fit_kwargs={'verbose': 0}    # Keyword arguments to pass to model.fit.\n",
    ")\n",
    "\n",
    "\n",
    "# create a dictionary, this is what sklearn wants\n",
    "\n",
    "# note that batch_size and epochs are not arguments of\n",
    "# our model construction function, but they are\n",
    "# normally arguments of model.fit() in Keras,\n",
    "# see what you did at the top of the notebook.\n",
    "\n",
    "# The names on the left here NEED to match the argument names\n",
    "# of your build_neural_net function.\n",
    "param_grid = dict(\n",
    "    hiddenLayerOne=[5, 10, 50],\n",
    "    hiddenLayerTwo=[5, 10, 50],\n",
    "    learnRate=[1e-2, 1e-3],\n",
    "    batch_size=[4, 8, 16],\n",
    "    epochs=[4, 8, 16, 32]\n",
    ")\n",
    "\n",
    "# Set up the randomized search with 3-fold cross-validation and 5 iterations.\n",
    "#-1 = use all CPUs, cv--> 3-fold cross-val.\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_distributions=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    n_iter=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model using randomized search.\n",
    "searchResults = search.fit(x_train, y_train)\n",
    "\n",
    "# Extract and display the best score and hyperparameters.\n",
    "bestScore = searchResults.best_score_\n",
    "bestParams = searchResults.best_params_\n",
    "print(\"Best score is {:.2f} using {}\".format(bestScore, bestParams))\n",
    "\n",
    "#finally, use best model on train data on test data (normally would use nested cross-validation)\n",
    "bestModel = searchResults.best_estimator_\n",
    "accuracy = bestModel.score(x_test, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7531a12",
   "metadata": {},
   "source": [
    "## What I want you to remember here\n",
    "\n",
    "There is of course a lot more nuance and power in Keras, this is just the tip of the iceberg. But the practical skills you need to take from here:\n",
    "* You know how to build and train a neural network model using Keras (both Dense and Convolutional models).\n",
    "* You know how to wrap a Keras model in a constructor function, and instantiate it as a sklearn model, so that you can use it with sklearn for hyperparameter optimalisation\n",
    "\n",
    "## The end\n",
    "\n",
    "With that, the course content is done. All that's left is to apply ML to an example dataset using your understanding of the fundamentals of what's _really_ going on, and the high-level high-powered libraries that you can now command. \n",
    "\n",
    "## Survey\n",
    "[Click me, click me. Oh, oh, oh click me!](https://docs.google.com/forms/d/e/1FAIpQLScURtN2iRNJrLGHqYLxOOZ8Fz8xjoqsIcC_lGzfwuukRvZ0Ew/viewform?usp=sf_link) - after Donkey (on the Shrek DVD main menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7323a9-c4b3-483c-84ec-000a56488ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.wrappers.SKLearnClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
