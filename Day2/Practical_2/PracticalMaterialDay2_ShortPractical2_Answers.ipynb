{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 2\n",
    "\n",
    "Welcome to the second practical of today. Here, you will work on implementing regularised logistic regression, as well as implementing cross-validation on some data and making an ROC curve. First run the two cells below to set things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1 / (1 + np.exp(-data))\n",
    "    return output\n",
    "\n",
    "\n",
    "# I have redefined the mySigmoid for numerical stability here.\n",
    "# Why this? Well, with many parameters and large values, numerical precision for the power function becomes\n",
    "# an issue. Read here: https://github.com/scipy/scipy/blob/91a279ecb05e7814e2787bfa618d46ad3e0af2be/scipy/special/_logit.h\n",
    "# how scipy fixes that.\n",
    "def mySigmoid(data):\n",
    "    data = np.array(data)\n",
    "    return expit(data)\n",
    "\n",
    "\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    # make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def linAlgLogRegHypothesis(data, thetas):\n",
    "    output = mySigmoid(linAlgRegHypothesis(data, thetas))\n",
    "    return output\n",
    "\n",
    "\n",
    "def costFuncLogReg(x, y, thetas):\n",
    "    predictions = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    totalCosts = np.nansum(1 / len(x) * costsPerSample)\n",
    "    return totalCosts\n",
    "\n",
    "\n",
    "def makeCrossValData(featureArray, y, k=10):\n",
    "    \"\"\"function to make splits into training and validation sets.\n",
    "    Outputs two lists of length k, where each element is the indices of samples to train on for that fold,\n",
    "    and the indices of samples to test on for that fold, respectively.\"\"\"\n",
    "    m = len(featureArray)\n",
    "    # shuffle data\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    shuffled_features = featureArray[shuffled_indices, :]  # assumes 2D array\n",
    "    shuffled_labels = y[shuffled_indices]\n",
    "    # see how many equal-sized sets you can make\n",
    "    dataPerSplit = int(np.floor(m / k))\n",
    "    dataPartitions = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        # make a list of all the samples for each fold\n",
    "        dataPartitions.append(list(range(counter, counter + dataPerSplit)))\n",
    "        counter += dataPerSplit\n",
    "\n",
    "    samplesEquallySplit = k * dataPerSplit\n",
    "    if not samplesEquallySplit == m:\n",
    "        # after making equal splits there will be samples left, i.e. you cannot always make k exactly evenly sized subsets.\n",
    "        # randomly assign left over samples to folds after\n",
    "        toDivide = m - samplesEquallySplit\n",
    "        for extraSampleIndex in range(counter, counter + toDivide):\n",
    "            # only assign to lists of samples that have the current minimum amount of samples\n",
    "            currentSubsetSizes = np.array([len(subset) for subset in dataPartitions])\n",
    "            assignTo = np.random.choice(\n",
    "                np.where(currentSubsetSizes == np.min(currentSubsetSizes))[0]\n",
    "            )\n",
    "            dataPartitions[assignTo].append(extraSampleIndex)\n",
    "\n",
    "    # Now make the final cross-validation set: make k sets, each set has (k-1)/k folds to train on, and 1 fold to test on.\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for validationSetIndex in range(0, k):\n",
    "        # put 1 fold in the test set\n",
    "        testSet.append(dataPartitions[validationSetIndex])\n",
    "        # put all other folds in the train set\n",
    "        trainSet.append(dataPartitions.copy())\n",
    "        trainSet[validationSetIndex].pop(validationSetIndex)\n",
    "        # this line makes sure all training set indices are in one big list, rather than k-1 small lists.\n",
    "        trainSet[validationSetIndex] = [\n",
    "            item for sublist in trainSet[validationSetIndex] for item in sublist\n",
    "        ]\n",
    "\n",
    "    return shuffled_features, shuffled_labels, trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c72a",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation is a method of automatically constraining how much your model can (over)fit on the training data. We add some factor (regularisation weight $\\lambda$) times the sum of squares of the parameter (excluding the intercept ($\\theta_0$) to the cost function. In this way, the model cannot pick extremely large values for the parameters, i.e. when you have 100 features, the model is forced to only have high $\\theta$ parameters for those features that matter a lot for correct classification, while having extremely low or even 0 values for features that don't. Hence, regularisation also automatically selects features that are of importance to your problem: _feature selection_! (Strictly speaking, this holds only for when you penalise the absolute of the sum of the parameters, not when you penalise the square). Note that once you have trained the model and want to know the cost on the validation/test set, you should not use regularised cost: you care about your performance in the end (which you hope is better because you constrain the parameters during fitting).\n",
    "\n",
    "* To get started, change your costFuncLogReg to have an extra argument `lambda_ = 0` ( _ because lambda is a keword for anonymous functions), that, if set to a value higher than 0, causes regularisation to be performed.\n",
    "* Make sure to exclude the bias/intercept term ($\\theta_0$) from this. By convention this is not regularised.\n",
    "* While you are at it, also reorder the arguments to `thetas, x, y, lambda_=0` so it is easier to use another optimizer if we want to!\n",
    "\n",
    "Hint:\n",
    "* Remember that the regularised logistic regression cost function is:\n",
    "![APicture](RegLogRegEq.PNG) <div>\n",
    "You already had the first part implemented, you only need to add the second part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "def costFuncLogReg(thetas, x, y, lambda_=0):\n",
    "    m = len(x)\n",
    "    predictions = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    # set the bias to 0 so you don't count it in regularisation.\n",
    "    # It is convention not to regularise the bias/intercept\n",
    "    thetas[0] = 0\n",
    "    regCost = lambda_ / 2 * 1 / m * thetas.T @ thetas\n",
    "    totalCosts = np.nansum(1 / m * costsPerSample) + regCost\n",
    "    return totalCosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8ee45",
   "metadata": {},
   "source": [
    "## Changing gradient descent\n",
    "\n",
    "The gradients should also change. Luckily, since all that's added is a plus term, the change is extremely minor:\n",
    "![gradients_logreg](GradientsRegLogReg.PNG)\n",
    "\n",
    "* Up to you to implement the changes in the `linAlgGradientDescent` function. Add another `lambda_ = 0` argument and change the gradients as needed. So `linAlgGradientDescent(x, y, thetas, alpha, lambda_ = 0, method = \"linear\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91272388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old function\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, method=\"linear\"):\n",
    "    possible_methods = [\"linear\", \"logistic\"]\n",
    "    if method not in possible_methods:\n",
    "        print(\n",
    "            \"Error! Wrong method given. Should be one of: \"\n",
    "            + str(possible_methods)\n",
    "            + \"\\n Returning None!\"\n",
    "        )\n",
    "        return\n",
    "    m = len(x)\n",
    "    ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.shape != (m, 1):\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha / m * gradientSummation\n",
    "    newThetas = thetas - finalGradientSteps.T\n",
    "    return newThetas\n",
    "\n",
    "\n",
    "# answer\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, lambda_=0, method=\"linear\"):\n",
    "    possible_methods = [\"linear\", \"logistic\"]\n",
    "    if method not in possible_methods:\n",
    "        print(\n",
    "            \"Error! Wrong method given. Should be one of: \"\n",
    "            + str(possible_methods)\n",
    "            + \"\\n Returning None!\"\n",
    "        )\n",
    "        return\n",
    "    m = len(x)\n",
    "    ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.shape != (m, 1):\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    gradientSummation[1:] += lambda_ / m * thetas[1:]\n",
    "    finalGradientSteps = alpha / m * gradientSummation\n",
    "    newThetas = thetas - finalGradientSteps.T\n",
    "    return newThetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb46850-3ffa-4614-852d-06379c4379a0",
   "metadata": {},
   "source": [
    "# Refactoring into two separate functions\n",
    "\n",
    "Below, I have made one function called `computeGradients()` that computes and returns the gradients, and another function called `gradientDescentStep()` that takes a step using current thetas, those gradients, and an alpha value. In this way, we can use the first one if we want to use some other optimizer (which wants just the gradients), and the second one if we want to use gradient descent proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d303fd-52a2-49dd-b836-2d8532dd3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor into separate functions\n",
    "def computeGradients(thetas, x, y, lambda_=0, method=\"linear\"):\n",
    "    m = len(x)\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    unregularisedGradients = 1 / m * gradientSummation\n",
    "    regularisedGradients = np.ravel(unregularisedGradients)\n",
    "    regularisedGradients[1:] = (\n",
    "        regularisedGradients[1:] + lambda_ / m * np.ravel(thetas)[1:]\n",
    "    )\n",
    "    # print(\"final regularised gradients:\")\n",
    "    # print(regularisedGradients)\n",
    "    return regularisedGradients\n",
    "\n",
    "\n",
    "def gradientDescentStep(thetas, gradients, alpha):\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if gradients.ndim < 2:\n",
    "        gradients = gradients[:, np.newaxis]\n",
    "    newThetas = thetas - alpha * gradients\n",
    "    return newThetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32fd36",
   "metadata": {},
   "source": [
    "## Loading in some data \n",
    "\n",
    "Let's look at the Pima Indians dataset, which contains information on multiple clinical variables and whether or not patients have diabetes. The below code loads in the data. I am using pandas since it has a nice .describe() method for DataFrame that shows you information about the data. Up to you to investigate this data somewhat:\n",
    "\n",
    "* Are there any NaNs in the data?\n",
    "* Are there other values that seem circumspect? Name 2 examples. How many of these circumspect values are there in these features?\n",
    "* How many cases and controls are there? Is this a balanced dataset?\n",
    "\n",
    "Hint(s):\n",
    "* Use the `.describe` method of the dataframe to help you answer these questions.\n",
    "* You can slice a dataframe using `df.loc[df[\"colName\"] < 12, :]`, which corresponds to getting you only rows for which the values in column _colName_ are less than 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d804a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of NaNs:\n",
      "No. cases BMI not measured:\n",
      "11\n",
      "No. cases SkinThickness not measured:\n",
      "227\n",
      "No. cases Insulin not measured:\n",
      "374\n",
      "Nr. of cases: \n",
      "268\n",
      "Nr. of controls: \n",
      "500\n",
      "The dataset is unbalanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "diabetesData = pd.read_csv(\"PimaIndiansDiabetes.csv\")\n",
    "\n",
    "# answer\n",
    "display(diabetesData.describe())\n",
    "# there are no NaNs\n",
    "print(\"nr of NaNs:\")\n",
    "np.sum(np.isnan(diabetesData))\n",
    "# there are indeed circumspect values. A SkinThickness of 0 or glucose level of 0 or\n",
    "# even a BMI of 0 is probably not correct.\n",
    "# we'll need to do something about this.\n",
    "print(\"No. cases BMI not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"BMI\"] <= 0, :]))\n",
    "print(\"No. cases SkinThickness not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"SkinThickness\"] <= 0, :]))\n",
    "print(\"No. cases Insulin not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"Insulin\"] <= 0, :]))\n",
    "# etc.\n",
    "\n",
    "# cases and controls\n",
    "print(\"Nr. of cases: \")\n",
    "print(len(diabetesData.loc[diabetesData[\"Outcome\"] == 1, :]))\n",
    "print(\"Nr. of controls: \")\n",
    "print(len(diabetesData.loc[diabetesData[\"Outcome\"] == 0, :]))\n",
    "print(\"The dataset is unbalanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a8b1",
   "metadata": {},
   "source": [
    "## Cleaning up the dataset\n",
    "\n",
    "# Note: this was removed from the practical. You can skip this.\n",
    "\n",
    "The dirty secret of ML is that you spend most of your time cleaning data. So you'll have to spend some time on that here. Do the following:\n",
    "\n",
    "* Replace the 0 values with `np.nan` (**Note**: be aware that you shouldn't do this for all columns. Think about it.)\n",
    "* Use [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) to impute values that are missing for those columns where you inserted NaNs. Those who have followed the BiBC Essentials Course might remember K-Nearest Neighbour clustering. This function determines the (by default) 5 most similar samples (based on data that is _not_ missing) and sets the bmi/glucose level, etc. to the mean of their values. Euclidean distance is used. We will discuss K-Nearest Neighbour clustering in two days. For now, you can just use it. To do so, use `a = KNNImputer(missing_values = np.nan)` followed by `imputedData = a.fit_transform(nonImputedData)`.\n",
    "* Note that this turns the DataFrame into a numpy array: this is not a problem but it's good to know.\n",
    "* Mean-normalise (i.e. subtract the mean and divide by the standard deviation) the features using the function provided below. This should be done on all the data except the labels.\n",
    "* Put the class into a `np.array` (a column vector) called `diabetesClassLabels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8e1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0      148             72             35        0  33.6   \n",
       "1       85             66             29        0  26.6   \n",
       "2      183             64              0        0  23.3   \n",
       "3       89             66             23       94  28.1   \n",
       "4      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0    148.0           72.0           35.0      NaN  33.6   \n",
       "1     85.0           66.0           29.0      NaN  26.6   \n",
       "2    183.0           64.0            NaN      NaN  23.3   \n",
       "3     89.0           66.0           23.0     94.0  28.1   \n",
       "4    137.0           40.0           35.0    168.0  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  \n",
       "0                     0.627   50  \n",
       "1                     0.351   31  \n",
       "2                     0.672   32  \n",
       "3                     0.167   21  \n",
       "4                     2.288   33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data row 0 : [ 0.86624554 -0.02673413  0.62678221  0.16533769  0.16942344  0.46849198\n",
      "  1.4259954   0.63994726]\n",
      "Data row 1 : [-1.20105367 -0.51910379 -0.01025789 -0.95617978 -0.84811658 -0.36506078\n",
      " -0.19067191 -0.84488505]\n",
      "Data row 2 : [ 2.0147451  -0.68322701 -0.60482866  0.22222626 -1.32781402  0.60439732\n",
      " -0.10558415  1.23388019]\n",
      "Data row 3 : [-1.06979658 -0.51910379 -0.647298   -0.59656277 -0.63007229 -0.92076261\n",
      " -1.04154944 -0.84488505]\n",
      "Data row 4 : [ 0.50528853 -2.65270563  0.62678221  0.15517902  1.55037062  5.4849091\n",
      " -0.0204964  -1.14185152]\n",
      "Data row 5 : [-0.1838112   0.13738909 -0.94458338 -0.55592808 -0.99347944 -0.81807858\n",
      " -0.27575966  0.3429808 ]\n",
      "Data row 6 : [-1.43075358 -1.83208954  0.30826216 -0.65751481 -0.20852    -0.676133\n",
      " -0.61611067 -0.25095213]\n",
      "Data row 7 : [-0.21662548 -0.32215593 -0.05272723 -0.29992952  0.41654031 -1.02042653\n",
      " -0.36084741  1.82781311]\n",
      "Data row 8 : [ 2.47414493 -0.19085735  1.68851572  3.96468131 -0.28120143 -0.94794368\n",
      "  1.68125866 -0.54791859]\n",
      "Data row 9 : [ 0.11151726  1.94274449 -0.28630861  0.13282994  0.36130242 -0.7244549\n",
      "  1.76634642  1.23388019]\n",
      "Mean of Pregnancies: 0.0\n",
      "Mean of Glucose: 0.0\n",
      "Mean of BloodPressure: -0.0\n",
      "Mean of SkinThickness: -0.0\n",
      "Mean of Insulin: -0.0\n",
      "Mean of BMI: 0.0\n",
      "Mean of DiabetesPedigreeFunction: 0.0\n",
      "Mean of Age: -0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def createNormalisedFeatures(featureArray, mode=\"range\", printit=False):\n",
    "    if printit:\n",
    "        print(featureArray)\n",
    "    featureMeans = np.mean(featureArray, axis=0, keepdims=True)\n",
    "    if printit:\n",
    "        print(featureMeans)\n",
    "    if printit:\n",
    "        print(featureArray - featureMeans)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis=0, keepdims=True) - np.min(\n",
    "            featureArray, axis=0, keepdims=True\n",
    "        )\n",
    "        # broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis=0, keepdims=True)\n",
    "        # broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "# answer\n",
    "# replace 0 with Nan. Don't do this for pregnancies: for all you know it could be a real 0 there.\n",
    "# also not for having diabetes or not, of course!\n",
    "dfSubset = diabetesData.iloc[:, 1:-1]\n",
    "display(dfSubset.head())\n",
    "dfSubsetNan = dfSubset.replace(0, value=np.nan)\n",
    "display(dfSubsetNan.head())\n",
    "# impute values\n",
    "imputer = KNNImputer(missing_values=np.nan)\n",
    "dfSubsetNan = imputer.fit_transform(dfSubsetNan)\n",
    "\n",
    "# add back the pregnancies column\n",
    "featuresDiabetesNotNorm = np.append(\n",
    "    dfSubsetNan, np.array(diabetesData[\"Pregnancies\"])[:, np.newaxis], axis=1\n",
    ")\n",
    "\n",
    "# normalise\n",
    "\n",
    "normDiabFeats, meansDiabFeats, standardDevsDiabFeats = createNormalisedFeatures(\n",
    "    featuresDiabetesNotNorm, \"SD\"\n",
    ")\n",
    "for index, row in enumerate(normDiabFeats[0:10]):\n",
    "    print(f\"Data row {index} : {row}\")\n",
    "for colname, mean in zip(diabetesData.columns, np.mean(normDiabFeats, axis=0)):\n",
    "    print(f\"Mean of {colname}: {np.round(mean)}\")\n",
    "diabetesClassLabels = np.array(diabetesData[\"Outcome\"])[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95721312-adbb-4ba6-a016-268d96337cb4",
   "metadata": {},
   "source": [
    "# Crappier data\n",
    "\n",
    "Hey, you know how you just cleaned some data? Good on you! However, for demonstration purposes to show regularisation in action, it's actually good to have some data that you might overfit on. To do that, I've asked your good friend ChatGPT-4O, and it modified the dataset to include:\n",
    "* Correlated (Redundant) Features: Duplicates of existing features with slight noise.\n",
    "* Irrelevant Noisy Features: Five random noise columns.\n",
    "* Polynomial and Interaction Features: Nonlinear relationships (e.g., Glucose × Insulin).\n",
    "* Weakly Predictive Features: Slightly correlated to the target variable with added noise.\n",
    "* Scaling Inconsistencies: A feature (BMI) scaled by 1000.\n",
    "\n",
    "We'll work with that, for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a424b-d6b9-45d1-acd0-87f60fc0287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final data first 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.86887702, -0.03237117,  0.65922142,  0.81046632,  0.1709203 ,\n",
       "         0.46849198,  1.4259954 ,  0.84809315,  0.14990472,  0.90853105],\n",
       "       [-1.19280439, -0.52466942,  0.03674349, -1.04856854, -0.84593101,\n",
       "        -0.36506078, -0.19067191, -1.12286242, -0.16082327,  0.53285613],\n",
       "       [ 2.01425557, -0.68876884, -0.77247782,  0.22048507, -1.32530376,\n",
       "         0.60439732, -0.10558415,  1.94391034, -0.26454839, -1.28785447],\n",
       "       [-1.06190398, -0.52466942, -0.58573444, -0.55559764, -0.6280343 ,\n",
       "        -0.92076261, -1.04154944, -0.99841719, -0.16203277,  0.15441653],\n",
       "       [ 0.5089009 , -2.65796183,  0.65922142,  0.17692941,  1.55093278,\n",
       "         5.4849091 , -0.0204964 ,  0.5042189 , -1.50470608,  0.90761447]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_parameters_messy: 274\n"
     ]
    }
   ],
   "source": [
    "pima_messy = pd.read_csv(\"PimaIndiansDiabetes_MessedUpNoise.csv\")\n",
    "\n",
    "# Ensure the dataset has an 'Outcome' column\n",
    "# Generate 50 'Weak_Signal' columns efficiently\n",
    "if \"Outcome\" in pima_messy.columns:\n",
    "    weak_signals = pima_messy[\"Outcome\"].values[\n",
    "        :, np.newaxis\n",
    "    ] * 0.05 + np.random.uniform(-1, 1, (pima_messy.shape[0], 50))\n",
    "    weak_signal_columns = [f\"Weak_Signal_{i}\" for i in range(1, 51)]\n",
    "\n",
    "    # Generate 200 random noise columns efficiently\n",
    "    random_noise = np.random.uniform(-1, 1, (pima_messy.shape[0], 200))\n",
    "    noise_columns = [f\"Random_Noise_{i}\" for i in range(1, 201)]\n",
    "\n",
    "    # Convert to DataFrames and concatenate efficiently\n",
    "    pima_messy = pd.concat(\n",
    "        [\n",
    "            pima_messy,\n",
    "            pd.DataFrame(weak_signals, columns=weak_signal_columns),\n",
    "            pd.DataFrame(random_noise, columns=noise_columns),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the normalization function\n",
    "def createNormalisedFeatures(featureArray, mode=\"range\", printit=False):\n",
    "    featureMeans = np.mean(featureArray, axis=0, keepdims=True)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis=0, keepdims=True) - np.min(\n",
    "            featureArray, axis=0, keepdims=True\n",
    "        )\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis=0, keepdims=True)\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "# Extract the 'Pregnancies' and 'Outcome' columns separately\n",
    "pregnancy_col = pima_messy[\"Pregnancies\"].values[:, np.newaxis]\n",
    "outcome_col = pima_messy[\"Outcome\"].values[:, np.newaxis]\n",
    "# print('Columns in data:\\n')\n",
    "# print('\\n'.join(pima_messy.columns),'\\n')\n",
    "\n",
    "# Replace 0 with NaN for imputation (excluding 'Pregnancies' and 'Outcome')\n",
    "dfSubsetNan_messy = pima_messy.drop(columns=[\"Pregnancies\", \"Outcome\"]).replace(\n",
    "    0, np.nan\n",
    ")\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(missing_values=np.nan)\n",
    "dfSubsetNan_messy = imputer.fit_transform(dfSubsetNan_messy)\n",
    "\n",
    "# Add back the 'Pregnancies' column\n",
    "featuresMessyNotNorm = np.append(dfSubsetNan_messy, pregnancy_col, axis=1)\n",
    "\n",
    "# Normalize using standard deviation method\n",
    "normMessyFeats, meansMessyFeats, standardDevsMessyFeats = createNormalisedFeatures(\n",
    "    featuresMessyNotNorm, \"SD\"\n",
    ")\n",
    "\n",
    "# Store class labels\n",
    "messyClassLabels = outcome_col\n",
    "\n",
    "print(\"final data first 5 rows:\")\n",
    "display(normMessyFeats[:5, :10])\n",
    "display(messyClassLabels[:5])\n",
    "n_parameters_messy = normMessyFeats.shape[1]\n",
    "print(f\"n_parameters_messy: {n_parameters_messy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9f384",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle I\n",
    "\n",
    "Okay, now we can train regularised logistic regression on this data. Let's **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**. We'll downsample the data so we have equal amounts of the positive and negative class, and train the classifier on 80% of the training data while testing on 20% held-out data (normally we'd use cross-validation but let's not put that extra level of complication in here as well). \n",
    "\n",
    "The visualisation of a decision boundary/what has been learned is somewhat complex: we can't just draw some boundary in 2D as our data isn't 2D but 8D.\n",
    "We'll reduce the dimensionality to two dimensions using PCA, and then show in those two dimensions which points are positive or negative for diabetes, and what the classifier predicts everywhere in that plane. This is done for you. We'll talk about dimensionality reduction on the last day of this week. For now, know that, by its nature, dimensionality reduction will lose some of the true differences in your data, so visualisation of the decision boundary in this 2D space is bound to be an approximation, and cannot capture completely what your classifier is doing (as it's separating things in 8 dimensions rather than 2)! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73704425-db92-4c68-a6b2-34eba4b0173e",
   "metadata": {},
   "source": [
    "Your job:\n",
    "\n",
    "* Downsample the normalised messy Diabetes Data (`normMessyFeats`): remove random rows of the controls so you have equal # of non-diabetes and diabetes cases. You could use `np.random.choice(a=rowIndicesOfoRowsThatDon'tHaveDiabetes, size = howManySamplesNeedToBeRemoved, replace = False)`, where you then remove (`np.delete()` can be useful) those rows from the feature and class label array. You'll probably also need `np.ravel(messyClassLabels)` and `np.where()`. <br> Save the new data as `equalClassSizeDiabetesData` and `equalClassSizeClassLabels` for the labels.\n",
    "Hints: \n",
    "* `np.where` returns a tuple, of which you need the first element.\n",
    "* Note that you can always insert a new cell above or below the current one for testing or debugging using `escape + a` or `escape + b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "# make sure everyone gets the same split\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# downsample the data\n",
    "\n",
    "\n",
    "# answer\n",
    "\n",
    "# downsampling\n",
    "whereClassLabelsAreNegative = np.where(np.ravel(messyClassLabels == 0))[0]\n",
    "nrPositiveLabels = len(messyClassLabels) - len(whereClassLabelsAreNegative)\n",
    "nrOfRowsToRemove = len(whereClassLabelsAreNegative) - nrPositiveLabels\n",
    "rowIndicesToRemove = np.random.choice(\n",
    "    a=whereClassLabelsAreNegative, size=nrOfRowsToRemove, replace=False\n",
    ")\n",
    "\n",
    "equalClassSizeDiabetesData = np.delete(\n",
    "    np.array(normMessyFeats), rowIndicesToRemove, axis=0\n",
    ")\n",
    "equalClassSizeClassLabels = np.delete(messyClassLabels, rowIndicesToRemove, axis=0)\n",
    "\n",
    "# check that it worked\n",
    "print(len(np.where(equalClassSizeClassLabels == 0)[0]))\n",
    "print(len(np.where(equalClassSizeClassLabels == 1)[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba13b",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle II\n",
    "\n",
    "* Make a list of the lambda values to train on (`lambdaValues`; **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**), an empty list to store the test cost in (`testCostList`), and a list for the final thetas after gradient descent (`finalThetaList`).\n",
    "* Randomly sample 80% of the data you produced above for training, and save the rest for testing. **_Code for this is given below!_**.\n",
    "* Now make a `for`-loop that loops over the different lambdaValues.\n",
    "* In that loop, make another loop that performs 300 gradient descent steps with an alpha of 0.2 on `trainDataDiabetes`.\n",
    "* After that's done, calculate the cost on `testDataDiabetes` **without regularisation (lambda of 0)**. Remember: you don't use the regularisation parameter in the final predictions, because you use it _during training_ to prevent overfitting, and then want to know how well you really do on the test data. \n",
    "* Append the result to the `testCostList`.\n",
    "* Finally, look at the DataFrame containing the theta parameters found for the different values of lambdas, and the cost calculated on the test set (code to make it is given below). What do you see? \n",
    "\n",
    "Hints:\n",
    "* There are many steps here. If you get stuck on one, ask a question or look at the answers to see how to do that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a85f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_value: 0.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-0.00087545 -0.00080227  0.00029556 -0.00089291  0.00080709]\n",
      "[-0.00020944 -0.00055231  0.00011994 -0.00031439  0.0005064 ]\n",
      "[-8.23514037e-05 -4.78992881e-04  7.13836391e-05 -8.51975617e-05\n",
      "  4.31684878e-04]\n",
      "[-4.30739581e-05 -4.40834655e-04  5.16542094e-05 -3.80573447e-06\n",
      "  3.93879373e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.558445\n",
      "         Iterations: 14\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n",
      "lambda_value: 0.5\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-0.00086389 -0.00077836  0.00028842 -0.00087261  0.00079481]\n",
      "[-0.00020217 -0.00052202  0.0001142  -0.00030006  0.00048903]\n",
      "[-7.77041563e-05 -4.42056118e-04  6.64779180e-05 -7.93929989e-05\n",
      "  4.07728137e-04]\n",
      "[-3.97313657e-05 -3.97528089e-04  4.70706431e-05 -3.45894660e-06\n",
      "  3.63482212e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.558890\n",
      "         Iterations: 14\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "lambda_value: 1.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-0.00085246 -0.00075509  0.00028145 -0.00085277  0.0007827 ]\n",
      "[-0.00019516 -0.00049331  0.00010873 -0.00028637  0.00047221]\n",
      "[-7.33158413e-05 -4.07895849e-04  6.19079286e-05 -7.39827387e-05\n",
      "  3.85055918e-04]\n",
      "[-3.66459718e-05 -3.58411650e-04  4.28929530e-05 -3.14379740e-06\n",
      "  3.35389711e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.559320\n",
      "         Iterations: 14\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 58\n",
      "lambda_value: 5.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-0.0007656  -0.00059043  0.00023133 -0.00070926  0.00069138]\n",
      "[-1.46927302e-04 -3.11876135e-04  7.33606988e-05 -1.97066168e-04\n",
      "  3.55816846e-04]\n",
      "[-4.59779689e-05 -2.12877439e-04  3.49895432e-05 -4.20406413e-05\n",
      "  2.42677830e-04]\n",
      "[-1.91546463e-05 -1.55402782e-04  2.03780369e-05 -1.46470934e-06\n",
      "  1.75497257e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.562249\n",
      "         Iterations: 14\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 66\n",
      "lambda_value: 10.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-0.00066794 -0.00043019  0.00018091 -0.00056303  0.00059048]\n",
      "[-1.02742747e-04 -1.72701288e-04  4.47531116e-05 -1.23398621e-04\n",
      "  2.48053010e-04]\n",
      "[-2.55673828e-05 -9.25103986e-05  1.71104545e-05 -2.07121325e-05\n",
      "  1.35070340e-04]\n",
      "[-8.47275705e-06 -5.35566278e-05  8.02332966e-06 -5.64602414e-07\n",
      "  7.73729870e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.566957\n",
      "         Iterations: 13\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "lambda_value: 100.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-4.79503780e-05  9.39415628e-06  1.96465536e-06 -7.57564634e-06\n",
      "  2.59292120e-05]\n",
      "[-1.31919702e-07  1.02691738e-07  2.29653538e-09 -2.21303545e-08\n",
      "  1.95324366e-07]\n",
      "[-4.96420417e-10  8.53257935e-10  2.36280197e-11 -4.45777564e-11\n",
      "  1.55094270e-09]\n",
      "[-2.33291597e-12  6.72659498e-12  2.86393739e-13 -2.53703308e-14\n",
      "  1.22178383e-11]\n",
      "         Current function value: 0.585472\n",
      "         Iterations: 2\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 77\n",
      "lambda_value: 1000.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n",
      "[-1.06221524e-07 -5.73215011e-07 -2.71511117e-07 -3.95622745e-07\n",
      " -5.83061165e-07]\n",
      "[-2.17836343e-13 -1.20391197e-12 -5.70241770e-13 -8.30908259e-13\n",
      " -1.22457600e-12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.62015104e-17 -2.77555756e-17  0.00000000e+00 -2.42861287e-17\n",
      " -4.85722573e-17]\n",
      "[-2.58191401e-19  1.38777878e-17  2.77555756e-17  1.73472348e-17\n",
      "  4.85722573e-17]\n",
      "         Current function value: 0.631184\n",
      "         Iterations: 2\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n",
      "lambda_value: 10000.0\n",
      "[-0.5        -0.32802865 -0.11852858 -0.16675311 -0.2383542 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
      "/var/folders/_h/r7771tk92552ms71w8h1mdwm0000gq/T/ipykernel_84208/359908367.py:16: RuntimeWarning: overflow encountered in matmul\n",
      "  gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n",
      "/var/folders/_h/r7771tk92552ms71w8h1mdwm0000gq/T/ipykernel_84208/2434493420.py:13: RuntimeWarning: invalid value encountered in matmul\n",
      "  predictions = newFeatArray @ thetas\n",
      "/var/folders/_h/r7771tk92552ms71w8h1mdwm0000gq/T/ipykernel_84208/359908367.py:16: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.57168997e+68 -6.92076643e+69 -3.28103680e+69 -4.78223214e+69\n",
      " -7.04611910e+69]\n",
      "[-9.25551330e+138 -1.79341485e+140 -8.50232747e+139 -1.23924682e+140\n",
      " -1.82589864e+140]\n",
      "[-2.39843140e+209 -4.64737326e+210 -2.20325427e+210 -3.21132754e+210\n",
      " -4.73155025e+210]\n",
      "[-6.21518548e+279 -1.20429906e+281 -5.70941238e+280 -8.32168737e+280\n",
      " -1.22611230e+281]\n",
      "         Current function value: 0.683166\n",
      "         Iterations: 1\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dstoker/opt/anaconda3/envs/BMLB2025_2/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1292: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta_0</th>\n",
       "      <th>theta_1</th>\n",
       "      <th>theta_2</th>\n",
       "      <th>theta_3</th>\n",
       "      <th>theta_4</th>\n",
       "      <th>theta_5</th>\n",
       "      <th>theta_6</th>\n",
       "      <th>theta_7</th>\n",
       "      <th>theta_8</th>\n",
       "      <th>theta_9</th>\n",
       "      <th>...</th>\n",
       "      <th>theta_266</th>\n",
       "      <th>theta_267</th>\n",
       "      <th>theta_268</th>\n",
       "      <th>theta_269</th>\n",
       "      <th>theta_270</th>\n",
       "      <th>theta_271</th>\n",
       "      <th>theta_272</th>\n",
       "      <th>theta_273</th>\n",
       "      <th>theta_274</th>\n",
       "      <th>testSetCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118028</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.063668</td>\n",
       "      <td>-0.034425</td>\n",
       "      <td>0.101095</td>\n",
       "      <td>0.093836</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>-0.023175</td>\n",
       "      <td>-0.021165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>-0.020090</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.046927</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.088018</td>\n",
       "      <td>0.673032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114736</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.062766</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>0.098121</td>\n",
       "      <td>0.090547</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>-0.020456</td>\n",
       "      <td>-0.021692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010062</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.046774</td>\n",
       "      <td>-0.027427</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>0.087622</td>\n",
       "      <td>0.672653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111683</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.061892</td>\n",
       "      <td>-0.030537</td>\n",
       "      <td>0.095331</td>\n",
       "      <td>0.087493</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>-0.017917</td>\n",
       "      <td>-0.022182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>-0.020098</td>\n",
       "      <td>0.011857</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.046621</td>\n",
       "      <td>-0.026616</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.033780</td>\n",
       "      <td>0.087232</td>\n",
       "      <td>0.672298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.055813</td>\n",
       "      <td>-0.018857</td>\n",
       "      <td>0.078226</td>\n",
       "      <td>0.069533</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>-0.002616</td>\n",
       "      <td>-0.025012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>-0.020003</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.045388</td>\n",
       "      <td>-0.021457</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.035090</td>\n",
       "      <td>0.084314</td>\n",
       "      <td>0.670114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081286</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.049998</td>\n",
       "      <td>-0.009896</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.012412</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>-0.026804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.019750</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>-0.017243</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.035786</td>\n",
       "      <td>0.081119</td>\n",
       "      <td>0.668424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051927</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.030647</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>-0.022674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>-0.016077</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>-0.003777</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.055755</td>\n",
       "      <td>0.666765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030983</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.013758</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.016698</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>0.028287</td>\n",
       "      <td>-0.004613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>-0.005402</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>-0.000831</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.681090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         theta_0   theta_1   theta_2   theta_3   theta_4   theta_5   theta_6  \\\n",
       "0.0          0.0  0.118028  0.001186  0.063668 -0.034425  0.101095  0.093836   \n",
       "0.5          0.0  0.114736  0.001651  0.062766 -0.032422  0.098121  0.090547   \n",
       "1.0          0.0  0.111683  0.002089  0.061892 -0.030537  0.095331  0.087493   \n",
       "5.0          0.0  0.093780  0.004796  0.055813 -0.018857  0.078226  0.069533   \n",
       "10.0         0.0  0.081286  0.006872  0.049998 -0.009896  0.065300  0.057037   \n",
       "100.0        0.0  0.051927  0.010398  0.024031  0.011501  0.034548  0.030647   \n",
       "1000.0       0.0  0.030983  0.007888  0.012252  0.013758  0.020206  0.016698   \n",
       "10000.0      0.0       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "          theta_7   theta_8   theta_9  ...  theta_266  theta_267  theta_268  \\\n",
       "0.0      0.009160 -0.023175 -0.021165  ...   0.009678  -0.020090   0.012477   \n",
       "0.5      0.009360 -0.020456 -0.021692  ...   0.010062  -0.020096   0.012161   \n",
       "1.0      0.009555 -0.017917 -0.022182  ...   0.010426  -0.020098   0.011857   \n",
       "5.0      0.010964 -0.002616 -0.025012  ...   0.012746  -0.020003   0.009804   \n",
       "10.0     0.012412  0.008651 -0.026804  ...   0.014641  -0.019750   0.007920   \n",
       "100.0    0.021169  0.035154 -0.022674  ...   0.018450  -0.016077  -0.000544   \n",
       "1000.0   0.019115  0.028287 -0.004613  ...   0.007589  -0.005402  -0.004005   \n",
       "10000.0       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "\n",
       "         theta_269  theta_270  theta_271  theta_272  theta_273  theta_274  \\\n",
       "0.0       0.004082   0.046927  -0.028285   0.004816   0.033285   0.088018   \n",
       "0.5       0.004227   0.046774  -0.027427   0.004479   0.033543   0.087622   \n",
       "1.0       0.004364   0.046621  -0.026616   0.004157   0.033780   0.087232   \n",
       "5.0       0.005218   0.045388  -0.021457   0.002021   0.035090   0.084314   \n",
       "10.0      0.005878   0.043911  -0.017243   0.000166   0.035786   0.081119   \n",
       "100.0     0.006666   0.030443  -0.003777  -0.003841   0.028700   0.055755   \n",
       "1000.0    0.003010   0.010283   0.000784  -0.000831   0.008912   0.023641   \n",
       "10000.0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "         testSetCost  \n",
       "0.0         0.673032  \n",
       "0.5         0.672653  \n",
       "1.0         0.672298  \n",
       "5.0         0.670114  \n",
       "10.0        0.668424  \n",
       "100.0       0.666765  \n",
       "1000.0      0.681090  \n",
       "10000.0          NaN  \n",
       "\n",
       "[8 rows x 276 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta_0</th>\n",
       "      <th>theta_1</th>\n",
       "      <th>theta_2</th>\n",
       "      <th>theta_3</th>\n",
       "      <th>theta_4</th>\n",
       "      <th>theta_5</th>\n",
       "      <th>theta_6</th>\n",
       "      <th>theta_7</th>\n",
       "      <th>theta_8</th>\n",
       "      <th>theta_9</th>\n",
       "      <th>...</th>\n",
       "      <th>theta_266</th>\n",
       "      <th>theta_267</th>\n",
       "      <th>theta_268</th>\n",
       "      <th>theta_269</th>\n",
       "      <th>theta_270</th>\n",
       "      <th>theta_271</th>\n",
       "      <th>theta_272</th>\n",
       "      <th>theta_273</th>\n",
       "      <th>theta_274</th>\n",
       "      <th>testSetCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.051130</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.052011</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>-0.034512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.016393</td>\n",
       "      <td>0.008773</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.047303</td>\n",
       "      <td>-0.014938</td>\n",
       "      <td>-0.002959</td>\n",
       "      <td>0.039575</td>\n",
       "      <td>0.089180</td>\n",
       "      <td>0.668755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079587</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.050661</td>\n",
       "      <td>-0.000631</td>\n",
       "      <td>0.051711</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>-0.034339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>-0.016447</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>-0.014751</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.088770</td>\n",
       "      <td>0.668579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078994</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.050060</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.048255</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>-0.034222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>-0.016527</td>\n",
       "      <td>0.008394</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>-0.014451</td>\n",
       "      <td>-0.003045</td>\n",
       "      <td>0.039447</td>\n",
       "      <td>0.088387</td>\n",
       "      <td>0.668382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075831</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>0.046742</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.049146</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>-0.033717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013956</td>\n",
       "      <td>-0.017574</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.045957</td>\n",
       "      <td>-0.012876</td>\n",
       "      <td>-0.003645</td>\n",
       "      <td>0.038765</td>\n",
       "      <td>0.085220</td>\n",
       "      <td>0.667445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067069</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.038619</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.039285</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>-0.032768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>-0.015364</td>\n",
       "      <td>0.006082</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.041325</td>\n",
       "      <td>-0.008039</td>\n",
       "      <td>-0.003274</td>\n",
       "      <td>0.041169</td>\n",
       "      <td>0.074797</td>\n",
       "      <td>0.663198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061577</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.025445</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.023869</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>-0.007153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>-0.020344</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.020573</td>\n",
       "      <td>0.022971</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.014637</td>\n",
       "      <td>0.046652</td>\n",
       "      <td>0.628437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.022370</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>0.030311</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.024171</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>-0.005356</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.024115</td>\n",
       "      <td>0.628403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.680067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         theta_0   theta_1   theta_2   theta_3   theta_4   theta_5   theta_6  \\\n",
       "0.0          0.0  0.080113  0.009063  0.051130 -0.000844  0.052011  0.048906   \n",
       "0.5          0.0  0.079587  0.009193  0.050661 -0.000631  0.051711  0.048559   \n",
       "1.0          0.0  0.078994  0.009316  0.050060 -0.000410  0.051299  0.048255   \n",
       "5.0          0.0  0.075831  0.009304  0.046742  0.000877  0.049146  0.046713   \n",
       "10.0         0.0  0.067069  0.012504  0.038619  0.011558  0.042982  0.039285   \n",
       "100.0        0.0  0.061577  0.014108  0.022639  0.025445  0.032597  0.023869   \n",
       "1000.0       0.0  0.045894  0.015681  0.022370  0.031209  0.030311  0.015423   \n",
       "10000.0      0.0  0.007735  0.002795  0.003932  0.005620  0.005286  0.002530   \n",
       "\n",
       "          theta_7   theta_8   theta_9  ...  theta_266  theta_267  theta_268  \\\n",
       "0.0      0.011254  0.012376 -0.034512  ...   0.012763  -0.016393   0.008773   \n",
       "0.5      0.011456  0.012832 -0.034339  ...   0.012970  -0.016447   0.008614   \n",
       "1.0      0.011742  0.013353 -0.034222  ...   0.013200  -0.016527   0.008394   \n",
       "5.0      0.012778  0.016482 -0.033717  ...   0.013956  -0.017574   0.006983   \n",
       "10.0     0.014540  0.024006 -0.032768  ...   0.016940  -0.015364   0.006082   \n",
       "100.0    0.036652  0.057787 -0.007153  ...   0.019919  -0.020344  -0.006218   \n",
       "1000.0   0.024171  0.044050  0.006243  ...   0.007480  -0.007574  -0.005356   \n",
       "10000.0  0.003981  0.007453  0.001393  ...   0.001044  -0.001053  -0.000924   \n",
       "\n",
       "         theta_269  theta_270  theta_271  theta_272  theta_273  theta_274  \\\n",
       "0.0       0.007625   0.047303  -0.014938  -0.002959   0.039575   0.089180   \n",
       "0.5       0.007614   0.047128  -0.014751  -0.002988   0.039493   0.088770   \n",
       "1.0       0.007698   0.046945  -0.014451  -0.003045   0.039447   0.088387   \n",
       "5.0       0.007795   0.045957  -0.012876  -0.003645   0.038765   0.085220   \n",
       "10.0      0.008514   0.041325  -0.008039  -0.003274   0.041169   0.074797   \n",
       "100.0     0.020573   0.022971   0.016019   0.004153   0.014637   0.046652   \n",
       "1000.0    0.005775   0.007335   0.007707   0.001657   0.006882   0.024115   \n",
       "10000.0   0.000692   0.000948   0.001174   0.000237   0.001041   0.003751   \n",
       "\n",
       "         testSetCost  \n",
       "0.0         0.668755  \n",
       "0.5         0.668579  \n",
       "1.0         0.668382  \n",
       "5.0         0.667445  \n",
       "10.0        0.663198  \n",
       "100.0       0.628437  \n",
       "1000.0      0.628403  \n",
       "10000.0     0.680067  \n",
       "\n",
       "[8 rows x 276 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(900)\n",
    "startThetas = np.array([0] * (n_parameters_messy + 1))[:, np.newaxis]\n",
    "nSteps = 500\n",
    "alpha = 0.2\n",
    "\n",
    "# making lists\n",
    "\n",
    "\n",
    "#     code for dividing into 80% and 20%\n",
    "\n",
    "nrSamplesToTake = int(np.ceil(0.8 * np.sum(equalClassSizeClassLabels == 0)))\n",
    "negativeSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 0)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 1)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSamplesTrain, positiveClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTrain, negativeClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "trainDataDiabetes = np.vstack([positiveSamplesTrain, negativeSamplesTrain])\n",
    "trainClassLabelsDiabetes = np.vstack(\n",
    "    [positiveClassLabelsTrain, negativeClassLabelsTrain]\n",
    ")\n",
    "\n",
    "\n",
    "negativeSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 0))\n",
    "        if i not in negativeSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 1))\n",
    "        if i not in positiveSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSamplesTest, positiveClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTest, negativeClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "testDataDiabetes = np.vstack([positiveSamplesTest, negativeSamplesTest])\n",
    "testClassLabelsDiabetes = np.vstack([positiveClassLabelsTest, negativeClassLabelsTest])\n",
    "\n",
    "\n",
    "# your looping code, performing gradient descent for each lambda and\n",
    "# calculating the cost on the test set after it's done, should go here:\n",
    "\n",
    "\n",
    "#     code to make a final DataFrame to show what happens:\n",
    "#     Uncomment all this code at once by selecting it and pressing Ctrl + /\n",
    "\n",
    "# finalThetas = [np.ravel(elem) for elem in finalThetas]\n",
    "# dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.array(testCostList)])\n",
    "# columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "# columnNames.append(\"testSetCost\")\n",
    "# dataFrame.columns = columnNames\n",
    "# dataFrame.set_index(lambdaValues, inplace = True)\n",
    "# display(dataFrame)\n",
    "\n",
    "# answer\n",
    "\n",
    "# making lists\n",
    "lambdaValues = np.array([0.0, 0.5, 1, 5, 10, 100, 1000, 10000])\n",
    "testCostList = []\n",
    "finalThetaList = []\n",
    "finalThetaListBFGS = []\n",
    "testCostListBFGS = []\n",
    "\n",
    "# also keep thetas for each run along descent, this is extra in the answer. You only need to keep the final theta\n",
    "thetasAlongDescent = []\n",
    "thetasAlongDescentBFGS = []\n",
    "for lambda_ in lambdaValues:\n",
    "    print(\"lambda_value: \" + str(lambda_))\n",
    "    thetas = startThetas\n",
    "    # print(thetas)\n",
    "    thetasAlongDescent.append([thetas])\n",
    "    for step in range(0, nSteps):\n",
    "        gradients = computeGradients(\n",
    "            thetas, trainDataDiabetes, trainClassLabelsDiabetes, lambda_=lambda_\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(gradients[:5])\n",
    "        thetas = gradientDescentStep(thetas, gradients, alpha)\n",
    "        thetasAlongDescent[-1].append(thetas)\n",
    "    # once done with gradient descent, save resulting cost and thetas.\n",
    "    finalThetaList.append(thetas)\n",
    "    testCostList.append(\n",
    "        costFuncLogReg(thetas, testDataDiabetes, testClassLabelsDiabetes, lambda_=0)\n",
    "    )\n",
    "\n",
    "    # Also calculate BFGS thetas for reference\n",
    "    thetasBFGS = fmin_bfgs(\n",
    "        costFuncLogReg,\n",
    "        np.ravel(startThetas),\n",
    "        computeGradients,\n",
    "        (trainDataDiabetes, trainClassLabelsDiabetes, lambda_),\n",
    "        retall=True,\n",
    "    )\n",
    "    thetasAlongDescentBFGS.append(thetasBFGS[1])\n",
    "    finalThetaListBFGS.append(thetasBFGS[0])\n",
    "    testCostListBFGS.append(\n",
    "        costFuncLogReg(\n",
    "            thetasBFGS[0], testDataDiabetes, testClassLabelsDiabetes, lambda_=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "# showing the results for gradient descent\n",
    "finalThetas = [np.ravel(elem[-1]) for elem in thetasAlongDescent]\n",
    "finalThetas = [np.ravel(elem) for elem in finalThetaList]\n",
    "dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.vstack(testCostList)])\n",
    "columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "columnNames.append(\"testSetCost\")\n",
    "dataFrame.columns = columnNames\n",
    "dataFrame.set_index(lambdaValues, inplace=True)\n",
    "display(dataFrame)\n",
    "\n",
    "# showing the result for min_bfgs\n",
    "finalThetas = [np.ravel(elem) for elem in finalThetaListBFGS]\n",
    "dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.vstack(testCostListBFGS)])\n",
    "columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "columnNames.append(\"testSetCost\")\n",
    "dataFrame.columns = columnNames\n",
    "dataFrame.set_index(lambdaValues, inplace=True)\n",
    "display(dataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bbb54",
   "metadata": {},
   "source": [
    "## Regularised logistic regression results\n",
    "\n",
    "If all goes well, you will see that the cost on the test set is lowest when using $\\lambda > 0$. Isn't that grand!? Not too strange when we all but forced overfitting by adding lots of uninformative features.  Spectacularly unconvincing example notwithstanding: in general, for unregularised classification, you run the risk of tuning the parameters _too specifically_ to the values in the training set, which increases the cost on the unseen test set. Regularisation guards against this by penalising too large parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd07cb2",
   "metadata": {},
   "source": [
    "## Classifier performance\n",
    "\n",
    "We've talked in the lectures about the performance of a classification algorithm. We want to know the true positive rate and false positive rates for a given threshold, but also the classifier's performance over a range of thresholds. It is not too difficult to make a ROC curve yourself. Let's do that now for the best classifier (with the lowest mean cost on the test set).\n",
    "\n",
    "Up to you to:\n",
    "* Make a range of 200 thresholds (from 1 to 0) for saying something is the positive set (use `np.linspace` for this).\n",
    "* Make two empty lists: `truePositiveRates` and `trueNegativeRates`.\n",
    "* Make predictions on the `testDataDiabetes` using the best set of learned thetas (which you can manually select or get from `finalThetas` using the index of the minimum element in `testSetCostList`).\n",
    "* Make a for loop over the different thresholds you defined. Within that loop:\n",
    "    * Turn the predictions into class labels using `np.where` and the current threshold value.\n",
    "    * Calculate the true positive rate (sensitivity/recall) and append it to the list.\n",
    "    * Calculate the true negative rate and append it to the `trueNegativeRates` list.\n",
    "* Finally make a plot of the sensitivity (true positive rate) on the y-axis and 1-specificity (1-TNR) on the x-axis. (use `fig, ax = plt.subplots()` and `ax.plot()`). Don't forget to set the axis labels and a title!\n",
    "\n",
    "See the relevant excerpt from the slide below, and look [here](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/) for more explanation if you want it! <br> ![SensitivityAndSpecificity](SensitivityAndSpecificity.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477afa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sensitivity')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# answer\n",
    "thresholdRange = np.linspace(1, 0, 200)\n",
    "truePositiveRate = []\n",
    "trueNegativeRate = []\n",
    "bestThetas = finalThetaList[np.nanargmin(testCostList)]\n",
    "predictionsBest = linAlgLogRegHypothesis(testDataDiabetes, bestThetas)\n",
    "\n",
    "for threshold in thresholdRange:\n",
    "    classLabelsThisThreshold = np.ravel(np.where(predictionsBest <= threshold, 0, 1))\n",
    "    truePositive = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 1)] == 1\n",
    "    )\n",
    "    falsePositive = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 0)] == 1\n",
    "    )\n",
    "    trueNegative = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 0)] == 0\n",
    "    )\n",
    "    falseNegative = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 1)] == 0\n",
    "    )\n",
    "    TPR = truePositive / (truePositive + falseNegative)\n",
    "    TNR = trueNegative / (trueNegative + falsePositive)\n",
    "    truePositiveRate.append(TPR)\n",
    "    trueNegativeRate.append(TNR)\n",
    "\n",
    "oneMinusSpecificity = [1 - elem for elem in trueNegativeRate]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(oneMinusSpecificity, truePositiveRate)\n",
    "ax.set_title(\"ROC curve for best thetas\")\n",
    "ax.set_xlabel(\"1-Specificity\")\n",
    "ax.set_ylabel(\"Sensitivity\")\n",
    "# ax.margins(x=0, y=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ed82",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What regularisation is, and how it works (penalising large weights for parameters, thereby forcing the algorithm to focus on those that really give it a lot of _bang for its buck_ and decreasing overfitting)\n",
    "* How to implement regularisation, and how the parameter $\\lambda$ affects it\n",
    "* How to do some basic cleaning on a dataset, and what _the idea_ of imputation is (specifically of a KNNImputer)\n",
    "* How to make a ROC plot, and what exactly is depicted on it, as well as why we might want to compare something like ROC AUC between classifiers, rather than accuracy. Note also that ROC AUC also compares wholly unrealistic thresholds (we would never use a threshold where we just say that everyone is positive or negative, say), so more clever comparison metrics exist. Note also that the ROC AUC does not, in and of itself, say much about out-of-domain generalisation, which might be most important in the real world!\n",
    "\n",
    "## Final words\n",
    "\n",
    "Congratulations. You've implemented regularised logistic regression on a real dataset (that you cleaned up yourself) and made your own ROC curve. We'll now move on to multiclass logistic regression and then to neural networks!\n",
    "\n",
    "## Survey\n",
    "\"I want a Survey, hey! Giving feedback for the very first time. I want a su-u-u-u-rvey, got some feedback, on my mi-i-i-n-d\". Thanks Weird Al, [very cool](https://www.youtube.com/watch?v=notKtAgfwDA). Here you go: [clickety-click](https://docs.google.com/forms/d/e/1FAIpQLSfaeqtRTz5KMqcmxQuOI5GYWHMejjh5_yuiCNSnNblpdKb0hQ/viewform?usp=sf_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
