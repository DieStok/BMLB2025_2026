{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 2\n",
    "\n",
    "Welcome to the second practical of today. Here, you will work on implementing regularised logistic regression, as well as implementing cross-validation on some data and making an ROC curve. First run the two cells below to set things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1 / (1 + np.exp(-data))\n",
    "    return output\n",
    "\n",
    "\n",
    "# I have redefined the mySigmoid for numerical stability here.\n",
    "# Why this? Well, with many parameters and large values, numerical precision for the power function becomes\n",
    "# an issue. Read here: https://github.com/scipy/scipy/blob/91a279ecb05e7814e2787bfa618d46ad3e0af2be/scipy/special/_logit.h\n",
    "# how scipy fixes that.\n",
    "def mySigmoid(data):\n",
    "    data = np.array(data)\n",
    "    return expit(data)\n",
    "\n",
    "\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    # make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def linAlgLogRegHypothesis(data, thetas):\n",
    "    output = mySigmoid(linAlgRegHypothesis(data, thetas))\n",
    "    return output\n",
    "\n",
    "\n",
    "def costFuncLogReg(x, y, thetas):\n",
    "    predictions = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    totalCosts = np.nansum(1 / len(x) * costsPerSample)\n",
    "    return totalCosts\n",
    "\n",
    "\n",
    "def makeCrossValData(featureArray, y, k=10):\n",
    "    \"\"\"function to make splits into training and validation sets.\n",
    "    Outputs two lists of length k, where each element is the indices of samples to train on for that fold,\n",
    "    and the indices of samples to test on for that fold, respectively.\"\"\"\n",
    "    m = len(featureArray)\n",
    "    # shuffle data\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    shuffled_features = featureArray[shuffled_indices, :]  # assumes 2D array\n",
    "    shuffled_labels = y[shuffled_indices]\n",
    "    # see how many equal-sized sets you can make\n",
    "    dataPerSplit = int(np.floor(m / k))\n",
    "    dataPartitions = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        # make a list of all the samples for each fold\n",
    "        dataPartitions.append(list(range(counter, counter + dataPerSplit)))\n",
    "        counter += dataPerSplit\n",
    "\n",
    "    samplesEquallySplit = k * dataPerSplit\n",
    "    if not samplesEquallySplit == m:\n",
    "        # after making equal splits there will be samples left, i.e. you cannot always make k exactly evenly sized subsets.\n",
    "        # randomly assign left over samples to folds after\n",
    "        toDivide = m - samplesEquallySplit\n",
    "        for extraSampleIndex in range(counter, counter + toDivide):\n",
    "            # only assign to lists of samples that have the current minimum amount of samples\n",
    "            currentSubsetSizes = np.array([len(subset) for subset in dataPartitions])\n",
    "            assignTo = np.random.choice(\n",
    "                np.where(currentSubsetSizes == np.min(currentSubsetSizes))[0]\n",
    "            )\n",
    "            dataPartitions[assignTo].append(extraSampleIndex)\n",
    "\n",
    "    # Now make the final cross-validation set: make k sets, each set has (k-1)/k folds to train on, and 1 fold to test on.\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for validationSetIndex in range(0, k):\n",
    "        # put 1 fold in the test set\n",
    "        testSet.append(dataPartitions[validationSetIndex])\n",
    "        # put all other folds in the train set\n",
    "        trainSet.append(dataPartitions.copy())\n",
    "        trainSet[validationSetIndex].pop(validationSetIndex)\n",
    "        # this line makes sure all training set indices are in one big list, rather than k-1 small lists.\n",
    "        trainSet[validationSetIndex] = [\n",
    "            item for sublist in trainSet[validationSetIndex] for item in sublist\n",
    "        ]\n",
    "\n",
    "    return shuffled_features, shuffled_labels, trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c72a",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation is a method of automatically constraining how much your model can (over)fit on the training data. We add some factor (regularisation weight $\\lambda$) times the sum of squares of the parameter (excluding the intercept ($\\theta_0$) to the cost function. In this way, the model cannot pick extremely large values for the parameters, i.e. when you have 100 features, the model is forced to only have high $\\theta$ parameters for those features that matter a lot for correct classification, while having extremely low or even 0 values for features that don't. Hence, regularisation also automatically selects features that are of importance to your problem: _feature selection_! (Strictly speaking, this holds only for when you penalise the absolute of the sum of the parameters, not when you penalise the square). Note that once you have trained the model and want to know the cost on the validation/test set, you should not use regularised cost: you care about your performance in the end (which you hope is better because you constrain the parameters during fitting).\n",
    "\n",
    "* To get started, change your costFuncLogReg to have an extra argument `lambda_ = 0` ( _ because lambda is a keword for anonymous functions), that, if set to a value higher than 0, causes regularisation to be performed.\n",
    "* Make sure to exclude the bias/intercept term ($\\theta_0$) from this. By convention this is not regularised.\n",
    "* While you are at it, also reorder the arguments to `thetas, x, y, lambda_=0` so it is easier to use another optimizer if we want to!\n",
    "\n",
    "Hint:\n",
    "* Remember that the regularised logistic regression cost function is:\n",
    "![APicture](RegLogRegEq.PNG) <div>\n",
    "You already had the first part implemented, you only need to add the second part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2aaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "def costFuncLogReg(thetas, x, y, lambda_=0):\n",
    "    m = len(x)\n",
    "    predictions = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    # set the bias to 0 so you don't count it in regularisation.\n",
    "    # It is convention not to regularise the bias/intercept\n",
    "    thetas[0] = 0\n",
    "    regCost = lambda_ / 2 * 1 / m * thetas.T @ thetas\n",
    "    totalCosts = np.nansum(1 / m * costsPerSample) + regCost\n",
    "    return totalCosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8ee45",
   "metadata": {},
   "source": [
    "## Changing gradient descent\n",
    "\n",
    "The gradients should also change. Luckily, since all that's added is a plus term, the change is extremely minor:\n",
    "![gradients_logreg](GradientsRegLogReg.PNG)\n",
    "\n",
    "* Up to you to implement the changes in the `linAlgGradientDescent` function. Add another `lambda_ = 0` argument and change the gradients as needed. So `linAlgGradientDescent(x, y, thetas, alpha, lambda_ = 0, method = \"linear\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91272388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old function\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, method=\"linear\"):\n",
    "    possible_methods = [\"linear\", \"logistic\"]\n",
    "    if method not in possible_methods:\n",
    "        print(\n",
    "            \"Error! Wrong method given. Should be one of: \"\n",
    "            + str(possible_methods)\n",
    "            + \"\\n Returning None!\"\n",
    "        )\n",
    "        return\n",
    "    m = len(x)\n",
    "    ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.shape != (m, 1):\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha / m * gradientSummation\n",
    "    newThetas = thetas - finalGradientSteps.T\n",
    "    return newThetas\n",
    "\n",
    "\n",
    "# answer\n",
    "# def linAlgGradientDescent(x, y, thetas, alpha, lambda_=0, method=\"linear\"):\n",
    "#     possible_methods = [\"linear\", \"logistic\"]\n",
    "#     if method not in possible_methods:\n",
    "#         print(\n",
    "#             \"Error! Wrong method given. Should be one of: \"\n",
    "#             + str(possible_methods)\n",
    "#             + \"\\n Returning None!\"\n",
    "#         )\n",
    "#         return\n",
    "#     m = len(x)\n",
    "#     ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "#     if thetas.ndim < 2:\n",
    "#         thetas = thetas[:, np.newaxis]\n",
    "#     if method == \"linear\":\n",
    "#         preds = linAlgRegHypothesis(x, thetas)\n",
    "#     else:\n",
    "#         preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "#     if preds.shape != (m, 1):\n",
    "#         preds = preds[:, np.newaxis]\n",
    "#     if y.shape != (m, 1):\n",
    "#         y = y[:, np.newaxis]\n",
    "#     errors = preds - y\n",
    "#     gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "#     gradientSummation[1:] += lambda_ / m * thetas[1:]\n",
    "#     finalGradientSteps = alpha / m * gradientSummation\n",
    "#     newThetas = thetas - finalGradientSteps.T\n",
    "#     return newThetas\n",
    "\n",
    "\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, lambda_=0, method=\"linear\"):\n",
    "    possible_methods = [\"linear\", \"logistic\"]\n",
    "    if method not in possible_methods:\n",
    "        print(\n",
    "            \"Error! Wrong method given. Should be one of: \"\n",
    "            + str(possible_methods)\n",
    "            + \"\\n Returning None!\"\n",
    "        )\n",
    "        return\n",
    "    m = len(x)\n",
    "    ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.shape != (m, 1):\n",
    "        y = y[:, np.newaxis]\n",
    "\n",
    "    errors = preds - y  # shape (m,1)\n",
    "\n",
    "    # build the augmented design matrix only for the gradient step\n",
    "    # example shapes:\n",
    "    #   errors.T: (1, m) like [[e1, e2, ..., em]]\n",
    "    #   np.c_[np.ones(m), x]: (m, n+1) like [[1, x11, x12, ... x1n],\n",
    "    #                                         [1, x21, x22, ... x2n],\n",
    "    #                                         ...\n",
    "    #                                         [1, xm1, xm2, ... xmn]]\n",
    "    #   result: (1, n+1) like [[Σe_i, Σ(e_i*x_i1), Σ(e_i*x_i2), ... Σ(e_i*x_in)]]\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "\n",
    "    # L2 regularization on non-bias terms (exclude the first column)\n",
    "    # example shapes:\n",
    "    #   gradientSummation[:, 1:]: (1, n)\n",
    "    #   thetas[1:].T:             (1, n)\n",
    "    gradientSummation[:, 1:] += lambda_ * thetas[1:].T\n",
    "\n",
    "    finalGradientSteps = alpha / m * gradientSummation  # scale by alpha/m\n",
    "    newThetas = thetas - finalGradientSteps.T  # (n+1,1) - (n+1,1)\n",
    "    return newThetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb46850-3ffa-4614-852d-06379c4379a0",
   "metadata": {},
   "source": [
    "# Refactoring into two separate functions\n",
    "\n",
    "Below, I have made one function called `computeGradients()` that computes and returns the gradients, and another function called `gradientDescentStep()` that takes a step using current thetas, those gradients, and an alpha value. In this way, we can use the first one if we want to use some other optimizer (which wants just the gradients), and the second one if we want to use gradient descent proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d303fd-52a2-49dd-b836-2d8532dd3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor into separate functions\n",
    "def computeGradients(thetas, x, y, lambda_=0, method=\"linear\"):\n",
    "    m = len(x)\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    unregularisedGradients = 1 / m * gradientSummation\n",
    "    regularisedGradients = np.ravel(unregularisedGradients)\n",
    "    regularisedGradients[1:] = (\n",
    "        regularisedGradients[1:] + lambda_ / m * np.ravel(thetas)[1:]\n",
    "    )\n",
    "    # print(\"final regularised gradients:\")\n",
    "    # print(regularisedGradients)\n",
    "    return regularisedGradients\n",
    "\n",
    "\n",
    "def gradientDescentStep(thetas, gradients, alpha):\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if gradients.ndim < 2:\n",
    "        gradients = gradients[:, np.newaxis]\n",
    "    newThetas = thetas - alpha * gradients\n",
    "    return newThetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32fd36",
   "metadata": {},
   "source": [
    "## Loading in some data \n",
    "\n",
    "Let's look at the Pima Indians dataset, which contains information on multiple clinical variables and whether or not patients have diabetes. The below code loads in the data. I am using pandas since it has a nice .describe() method for DataFrame that shows you information about the data. Up to you to investigate this data somewhat:\n",
    "\n",
    "* Are there any NaNs in the data?\n",
    "* Are there other values that seem circumspect? Name 2 examples. How many of these circumspect values are there in these features?\n",
    "* How many cases and controls are there? Is this a balanced dataset?\n",
    "\n",
    "Hint(s):\n",
    "* Use the `.describe` method of the dataframe to help you answer these questions.\n",
    "* You can slice a dataframe using `df.loc[df[\"colName\"] < 12, :]`, which corresponds to getting you only rows for which the values in column _colName_ are less than 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d804a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PimaIndiansDiabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m diabetesData = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPimaIndiansDiabetes.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# answer\u001b[39;00m\n\u001b[32m      4\u001b[39m display(diabetesData.describe())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/BMLB2025_2026/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/BMLB2025_2026/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/BMLB2025_2026/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/BMLB2025_2026/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/BMLB2025_2026/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'PimaIndiansDiabetes.csv'"
     ]
    }
   ],
   "source": [
    "diabetesData = pd.read_csv(\"PimaIndiansDiabetes.csv\")\n",
    "\n",
    "# answer\n",
    "display(diabetesData.describe())\n",
    "# there are no NaNs\n",
    "print(\"nr of NaNs:\")\n",
    "np.sum(np.isnan(diabetesData))\n",
    "# there are indeed circumspect values. A SkinThickness of 0 or glucose level of 0 or\n",
    "# even a BMI of 0 is probably not correct.\n",
    "# we'll need to do something about this.\n",
    "print(\"No. cases BMI not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"BMI\"] <= 0, :]))\n",
    "print(\"No. cases SkinThickness not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"SkinThickness\"] <= 0, :]))\n",
    "print(\"No. cases Insulin not measured:\")\n",
    "print(len(diabetesData.loc[diabetesData[\"Insulin\"] <= 0, :]))\n",
    "# etc.\n",
    "\n",
    "# cases and controls\n",
    "print(\"Nr. of cases: \")\n",
    "print(len(diabetesData.loc[diabetesData[\"Outcome\"] == 1, :]))\n",
    "print(\"Nr. of controls: \")\n",
    "print(len(diabetesData.loc[diabetesData[\"Outcome\"] == 0, :]))\n",
    "print(\"The dataset is unbalanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a8b1",
   "metadata": {},
   "source": [
    "## Cleaning up the dataset\n",
    "\n",
    "# Note: this was removed from the practical. You can skip this.\n",
    "\n",
    "The dirty secret of ML is that you spend most of your time cleaning data. So you'll have to spend some time on that here. Do the following:\n",
    "\n",
    "* Replace the 0 values with `np.nan` (**Note**: be aware that you shouldn't do this for all columns. Think about it.)\n",
    "* Use [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) to impute values that are missing for those columns where you inserted NaNs. Those who have followed the BiBC Essentials Course might remember K-Nearest Neighbour clustering. This function determines the (by default) 5 most similar samples (based on data that is _not_ missing) and sets the bmi/glucose level, etc. to the mean of their values. Euclidean distance is used. We will discuss K-Nearest Neighbour clustering in two days. For now, you can just use it. To do so, use `a = KNNImputer(missing_values = np.nan)` followed by `imputedData = a.fit_transform(nonImputedData)`.\n",
    "* Note that this turns the DataFrame into a numpy array: this is not a problem but it's good to know.\n",
    "* Mean-normalise (i.e. subtract the mean and divide by the standard deviation) the features using the function provided below. This should be done on all the data except the labels.\n",
    "* Put the class into a `np.array` (a column vector) called `diabetesClassLabels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def createNormalisedFeatures(featureArray, mode=\"range\", printit=False):\n",
    "    if printit:\n",
    "        print(featureArray)\n",
    "    featureMeans = np.mean(featureArray, axis=0, keepdims=True)\n",
    "    if printit:\n",
    "        print(featureMeans)\n",
    "    if printit:\n",
    "        print(featureArray - featureMeans)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis=0, keepdims=True) - np.min(\n",
    "            featureArray, axis=0, keepdims=True\n",
    "        )\n",
    "        # broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis=0, keepdims=True)\n",
    "        # broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "# answer\n",
    "# replace 0 with Nan. Don't do this for pregnancies: for all you know it could be a real 0 there.\n",
    "# also not for having diabetes or not, of course!\n",
    "dfSubset = diabetesData.iloc[:, 1:-1]\n",
    "display(dfSubset.head())\n",
    "dfSubsetNan = dfSubset.replace(0, value=np.nan)\n",
    "display(dfSubsetNan.head())\n",
    "# impute values\n",
    "imputer = KNNImputer(missing_values=np.nan)\n",
    "dfSubsetNan = imputer.fit_transform(dfSubsetNan)\n",
    "\n",
    "# add back the pregnancies column\n",
    "featuresDiabetesNotNorm = np.append(\n",
    "    dfSubsetNan, np.array(diabetesData[\"Pregnancies\"])[:, np.newaxis], axis=1\n",
    ")\n",
    "\n",
    "# normalise\n",
    "\n",
    "normDiabFeats, meansDiabFeats, standardDevsDiabFeats = createNormalisedFeatures(\n",
    "    featuresDiabetesNotNorm, \"SD\"\n",
    ")\n",
    "for index, row in enumerate(normDiabFeats[0:10]):\n",
    "    print(f\"Data row {index} : {row}\")\n",
    "for colname, mean in zip(diabetesData.columns, np.mean(normDiabFeats, axis=0)):\n",
    "    print(f\"Mean of {colname}: {np.round(mean)}\")\n",
    "diabetesClassLabels = np.array(diabetesData[\"Outcome\"])[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95721312-adbb-4ba6-a016-268d96337cb4",
   "metadata": {},
   "source": [
    "# Crappier data\n",
    "\n",
    "Hey, you know how you just cleaned some data? Good on you! However, for demonstration purposes to show regularisation in action, it's actually good to have some data that you might overfit on. To do that, I've asked your good friend ChatGPT-4O, and it modified the dataset to include:\n",
    "* Correlated (Redundant) Features: Duplicates of existing features with slight noise.\n",
    "* Irrelevant Noisy Features: Five random noise columns.\n",
    "* Polynomial and Interaction Features: Nonlinear relationships (e.g., Glucose × Insulin).\n",
    "* Weakly Predictive Features: Slightly correlated to the target variable with added noise.\n",
    "* Scaling Inconsistencies: A feature (BMI) scaled by 1000.\n",
    "\n",
    "We'll work with that, for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a424b-d6b9-45d1-acd0-87f60fc0287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_messy = pd.read_csv(\"PimaIndiansDiabetes_MessedUpNoise.csv\")\n",
    "\n",
    "# Ensure the dataset has an 'Outcome' column\n",
    "# Generate 50 'Weak_Signal' columns efficiently\n",
    "if \"Outcome\" in pima_messy.columns:\n",
    "    weak_signals = pima_messy[\"Outcome\"].values[\n",
    "        :, np.newaxis\n",
    "    ] * 0.05 + np.random.uniform(-1, 1, (pima_messy.shape[0], 50))\n",
    "    weak_signal_columns = [f\"Weak_Signal_{i}\" for i in range(1, 51)]\n",
    "\n",
    "    # Generate 200 random noise columns efficiently\n",
    "    random_noise = np.random.uniform(-1, 1, (pima_messy.shape[0], 200))\n",
    "    noise_columns = [f\"Random_Noise_{i}\" for i in range(1, 201)]\n",
    "\n",
    "    # Convert to DataFrames and concatenate efficiently\n",
    "    pima_messy = pd.concat(\n",
    "        [\n",
    "            pima_messy,\n",
    "            pd.DataFrame(weak_signals, columns=weak_signal_columns),\n",
    "            pd.DataFrame(random_noise, columns=noise_columns),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the normalization function\n",
    "def createNormalisedFeatures(featureArray, mode=\"range\", printit=False):\n",
    "    featureMeans = np.mean(featureArray, axis=0, keepdims=True)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis=0, keepdims=True) - np.min(\n",
    "            featureArray, axis=0, keepdims=True\n",
    "        )\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis=0, keepdims=True)\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "# Extract the 'Pregnancies' and 'Outcome' columns separately\n",
    "pregnancy_col = pima_messy[\"Pregnancies\"].values[:, np.newaxis]\n",
    "outcome_col = pima_messy[\"Outcome\"].values[:, np.newaxis]\n",
    "# print('Columns in data:\\n')\n",
    "# print('\\n'.join(pima_messy.columns),'\\n')\n",
    "\n",
    "# Replace 0 with NaN for imputation (excluding 'Pregnancies' and 'Outcome')\n",
    "dfSubsetNan_messy = pima_messy.drop(columns=[\"Pregnancies\", \"Outcome\"]).replace(\n",
    "    0, np.nan\n",
    ")\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(missing_values=np.nan)\n",
    "dfSubsetNan_messy = imputer.fit_transform(dfSubsetNan_messy)\n",
    "\n",
    "# Add back the 'Pregnancies' column\n",
    "featuresMessyNotNorm = np.append(dfSubsetNan_messy, pregnancy_col, axis=1)\n",
    "\n",
    "# Normalize using standard deviation method\n",
    "normMessyFeats, meansMessyFeats, standardDevsMessyFeats = createNormalisedFeatures(\n",
    "    featuresMessyNotNorm, \"SD\"\n",
    ")\n",
    "\n",
    "# Store class labels\n",
    "messyClassLabels = outcome_col\n",
    "\n",
    "print(\"final data first 5 rows:\")\n",
    "display(normMessyFeats[:5, :10])\n",
    "display(messyClassLabels[:5])\n",
    "n_parameters_messy = normMessyFeats.shape[1]\n",
    "print(f\"n_parameters_messy: {n_parameters_messy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9f384",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle I\n",
    "\n",
    "Okay, now we can train regularised logistic regression on this data. Let's **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**. We'll downsample the data so we have equal amounts of the positive and negative class, and train the classifier on 80% of the training data while testing on 20% held-out data (normally we'd use cross-validation but let's not put that extra level of complication in here as well). \n",
    "\n",
    "The visualisation of a decision boundary/what has been learned is somewhat complex: we can't just draw some boundary in 2D as our data isn't 2D but 8D.\n",
    "We'll reduce the dimensionality to two dimensions using PCA, and then show in those two dimensions which points are positive or negative for diabetes, and what the classifier predicts everywhere in that plane. This is done for you. We'll talk about dimensionality reduction on the last day of this week. For now, know that, by its nature, dimensionality reduction will lose some of the true differences in your data, so visualisation of the decision boundary in this 2D space is bound to be an approximation, and cannot capture completely what your classifier is doing (as it's separating things in 8 dimensions rather than 2)! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73704425-db92-4c68-a6b2-34eba4b0173e",
   "metadata": {},
   "source": [
    "Your job:\n",
    "\n",
    "* Downsample the normalised messy Diabetes Data (`normMessyFeats`): remove random rows of the controls so you have equal # of non-diabetes and diabetes cases. You could use `np.random.choice(a=rowIndicesOfoRowsThatDon'tHaveDiabetes, size = howManySamplesNeedToBeRemoved, replace = False)`, where you then remove (`np.delete()` can be useful) those rows from the feature and class label array. You'll probably also need `np.ravel(messyClassLabels)` and `np.where()`. <br> Save the new data as `equalClassSizeDiabetesData` and `equalClassSizeClassLabels` for the labels.\n",
    "Hints: \n",
    "* `np.where` returns a tuple, of which you need the first element.\n",
    "* Note that you can always insert a new cell above or below the current one for testing or debugging using `escape + a` or `escape + b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure everyone gets the same split\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# downsample the data\n",
    "\n",
    "\n",
    "# answer\n",
    "\n",
    "# downsampling\n",
    "whereClassLabelsAreNegative = np.where(np.ravel(messyClassLabels == 0))[0]\n",
    "nrPositiveLabels = len(messyClassLabels) - len(whereClassLabelsAreNegative)\n",
    "nrOfRowsToRemove = len(whereClassLabelsAreNegative) - nrPositiveLabels\n",
    "rowIndicesToRemove = np.random.choice(\n",
    "    a=whereClassLabelsAreNegative, size=nrOfRowsToRemove, replace=False\n",
    ")\n",
    "\n",
    "equalClassSizeDiabetesData = np.delete(\n",
    "    np.array(normMessyFeats), rowIndicesToRemove, axis=0\n",
    ")\n",
    "equalClassSizeClassLabels = np.delete(messyClassLabels, rowIndicesToRemove, axis=0)\n",
    "\n",
    "# check that it worked\n",
    "print(len(np.where(equalClassSizeClassLabels == 0)[0]))\n",
    "print(len(np.where(equalClassSizeClassLabels == 1)[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba13b",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle II\n",
    "\n",
    "* Make a list of the lambda values to train on (`lambdaValues`; **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**), an empty list to store the test cost in (`testCostList`), and a list for the final thetas after gradient descent (`finalThetaList`).\n",
    "* Randomly sample 80% of the data you produced above for training, and save the rest for testing. **_Code for this is given below!_**.\n",
    "* Now make a `for`-loop that loops over the different lambdaValues.\n",
    "* In that loop, make another loop that performs 300 gradient descent steps with an alpha of 0.2 on `trainDataDiabetes`.\n",
    "* After that's done, calculate the cost on `testDataDiabetes` **without regularisation (lambda of 0)**. Remember: you don't use the regularisation parameter in the final predictions, because you use it _during training_ to prevent overfitting, and then want to know how well you really do on the test data. \n",
    "* Append the result to the `testCostList`.\n",
    "* Finally, look at the DataFrame containing the theta parameters found for the different values of lambdas, and the cost calculated on the test set (code to make it is given below). What do you see? \n",
    "\n",
    "Hints:\n",
    "* There are many steps here. If you get stuck on one, ask a question or look at the answers to see how to do that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(900)\n",
    "startThetas = np.array([0] * (n_parameters_messy + 1))[:, np.newaxis]\n",
    "nSteps = 500\n",
    "alpha = 0.2\n",
    "\n",
    "# making lists\n",
    "\n",
    "\n",
    "#     code for dividing into 80% and 20%\n",
    "\n",
    "nrSamplesToTake = int(np.ceil(0.8 * np.sum(equalClassSizeClassLabels == 0)))\n",
    "negativeSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 0)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 1)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSamplesTrain, positiveClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTrain, negativeClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "trainDataDiabetes = np.vstack([positiveSamplesTrain, negativeSamplesTrain])\n",
    "trainClassLabelsDiabetes = np.vstack(\n",
    "    [positiveClassLabelsTrain, negativeClassLabelsTrain]\n",
    ")\n",
    "\n",
    "\n",
    "negativeSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 0))\n",
    "        if i not in negativeSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 1))\n",
    "        if i not in positiveSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSamplesTest, positiveClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTest, negativeClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "testDataDiabetes = np.vstack([positiveSamplesTest, negativeSamplesTest])\n",
    "testClassLabelsDiabetes = np.vstack([positiveClassLabelsTest, negativeClassLabelsTest])\n",
    "\n",
    "\n",
    "# your looping code, performing gradient descent for each lambda and\n",
    "# calculating the cost on the test set after it's done, should go here:\n",
    "\n",
    "\n",
    "#     code to make a final DataFrame to show what happens:\n",
    "#     Uncomment all this code at once by selecting it and pressing Ctrl + /\n",
    "\n",
    "# finalThetas = [np.ravel(elem) for elem in finalThetas]\n",
    "# dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.array(testCostList)])\n",
    "# columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "# columnNames.append(\"testSetCost\")\n",
    "# dataFrame.columns = columnNames\n",
    "# dataFrame.set_index(lambdaValues, inplace = True)\n",
    "# display(dataFrame)\n",
    "\n",
    "# answer\n",
    "\n",
    "# making lists\n",
    "lambdaValues = np.array([0.0, 0.5, 1, 5, 10, 100, 1000, 10000])\n",
    "testCostList = []\n",
    "finalThetaList = []\n",
    "finalThetaListBFGS = []\n",
    "testCostListBFGS = []\n",
    "\n",
    "# also keep thetas for each run along descent, this is extra in the answer. You only need to keep the final theta\n",
    "thetasAlongDescent = []\n",
    "thetasAlongDescentBFGS = []\n",
    "for lambda_ in lambdaValues:\n",
    "    print(\"lambda_value: \" + str(lambda_))\n",
    "    thetas = startThetas\n",
    "    # print(thetas)\n",
    "    thetasAlongDescent.append([thetas])\n",
    "    for step in range(0, nSteps):\n",
    "        gradients = computeGradients(\n",
    "            thetas, trainDataDiabetes, trainClassLabelsDiabetes, lambda_=lambda_\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(gradients[:5])\n",
    "        thetas = gradientDescentStep(thetas, gradients, alpha)\n",
    "        thetasAlongDescent[-1].append(thetas)\n",
    "    # once done with gradient descent, save resulting cost and thetas.\n",
    "    finalThetaList.append(thetas)\n",
    "    testCostList.append(\n",
    "        costFuncLogReg(thetas, testDataDiabetes, testClassLabelsDiabetes, lambda_=0)\n",
    "    )\n",
    "\n",
    "    # Also calculate BFGS thetas for reference\n",
    "    thetasBFGS = fmin_bfgs(\n",
    "        costFuncLogReg,\n",
    "        np.ravel(startThetas),\n",
    "        computeGradients,\n",
    "        (trainDataDiabetes, trainClassLabelsDiabetes, lambda_),\n",
    "        retall=True,\n",
    "    )\n",
    "    thetasAlongDescentBFGS.append(thetasBFGS[1])\n",
    "    finalThetaListBFGS.append(thetasBFGS[0])\n",
    "    testCostListBFGS.append(\n",
    "        costFuncLogReg(\n",
    "            thetasBFGS[0], testDataDiabetes, testClassLabelsDiabetes, lambda_=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "# showing the results for gradient descent\n",
    "finalThetas = [np.ravel(elem[-1]) for elem in thetasAlongDescent]\n",
    "finalThetas = [np.ravel(elem) for elem in finalThetaList]\n",
    "dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.vstack(testCostList)])\n",
    "columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "columnNames.append(\"testSetCost\")\n",
    "dataFrame.columns = columnNames\n",
    "dataFrame.set_index(lambdaValues, inplace=True)\n",
    "display(dataFrame)\n",
    "\n",
    "# showing the result for min_bfgs\n",
    "finalThetas = [np.ravel(elem) for elem in finalThetaListBFGS]\n",
    "dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.vstack(testCostListBFGS)])\n",
    "columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "columnNames.append(\"testSetCost\")\n",
    "dataFrame.columns = columnNames\n",
    "dataFrame.set_index(lambdaValues, inplace=True)\n",
    "display(dataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bbb54",
   "metadata": {},
   "source": [
    "## Regularised logistic regression results\n",
    "\n",
    "If all goes well, you will see that the cost on the test set is lowest when using $\\lambda > 0$. Isn't that grand!? Not too strange when we all but forced overfitting by adding lots of uninformative features.  Spectacularly unconvincing example notwithstanding: in general, for unregularised classification, you run the risk of tuning the parameters _too specifically_ to the values in the training set, which increases the cost on the unseen test set. Regularisation guards against this by penalising too large parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd07cb2",
   "metadata": {},
   "source": [
    "## Classifier performance\n",
    "\n",
    "We've talked in the lectures about the performance of a classification algorithm. We want to know the true positive rate and false positive rates for a given threshold, but also the classifier's performance over a range of thresholds. It is not too difficult to make a ROC curve yourself. Let's do that now for the best classifier (with the lowest mean cost on the test set).\n",
    "\n",
    "Up to you to:\n",
    "* Make a range of 200 thresholds (from 1 to 0) for saying something is the positive set (use `np.linspace` for this).\n",
    "* Make two empty lists: `truePositiveRates` and `trueNegativeRates`.\n",
    "* Make predictions on the `testDataDiabetes` using the best set of learned thetas (which you can manually select or get from `finalThetas` using the index of the minimum element in `testSetCostList`).\n",
    "* Make a for loop over the different thresholds you defined. Within that loop:\n",
    "    * Turn the predictions into class labels using `np.where` and the current threshold value.\n",
    "    * Calculate the true positive rate (sensitivity/recall) and append it to the list.\n",
    "    * Calculate the true negative rate and append it to the `trueNegativeRates` list.\n",
    "* Finally make a plot of the sensitivity (true positive rate) on the y-axis and 1-specificity (1-TNR) on the x-axis. (use `fig, ax = plt.subplots()` and `ax.plot()`). Don't forget to set the axis labels and a title!\n",
    "\n",
    "See the relevant excerpt from the slide below, and look [here](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/) for more explanation if you want it! <br> ![SensitivityAndSpecificity](SensitivityAndSpecificity.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477afa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "thresholdRange = np.linspace(1, 0, 200)\n",
    "truePositiveRate = []\n",
    "trueNegativeRate = []\n",
    "bestThetas = finalThetaList[np.nanargmin(testCostList)]\n",
    "predictionsBest = linAlgLogRegHypothesis(testDataDiabetes, bestThetas)\n",
    "\n",
    "for threshold in thresholdRange:\n",
    "    classLabelsThisThreshold = np.ravel(np.where(predictionsBest <= threshold, 0, 1))\n",
    "    truePositive = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 1)] == 1\n",
    "    )\n",
    "    falsePositive = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 0)] == 1\n",
    "    )\n",
    "    trueNegative = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 0)] == 0\n",
    "    )\n",
    "    falseNegative = np.sum(\n",
    "        classLabelsThisThreshold[np.ravel(testClassLabelsDiabetes == 1)] == 0\n",
    "    )\n",
    "    TPR = truePositive / (truePositive + falseNegative)\n",
    "    TNR = trueNegative / (trueNegative + falsePositive)\n",
    "    truePositiveRate.append(TPR)\n",
    "    trueNegativeRate.append(TNR)\n",
    "\n",
    "oneMinusSpecificity = [1 - elem for elem in trueNegativeRate]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(oneMinusSpecificity, truePositiveRate)\n",
    "ax.set_title(\"ROC curve for best thetas\")\n",
    "ax.set_xlabel(\"1-Specificity\")\n",
    "ax.set_ylabel(\"Sensitivity\")\n",
    "# ax.margins(x=0, y=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ed82",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What regularisation is, and how it works (penalising large weights for parameters, thereby forcing the algorithm to focus on those that really give it a lot of _bang for its buck_ and decreasing overfitting)\n",
    "* How to implement regularisation, and how the parameter $\\lambda$ affects it\n",
    "* How to do some basic cleaning on a dataset, and what _the idea_ of imputation is (specifically of a KNNImputer)\n",
    "* How to make a ROC plot, and what exactly is depicted on it, as well as why we might want to compare something like ROC AUC between classifiers, rather than accuracy. Note also that ROC AUC also compares wholly unrealistic thresholds (we would never use a threshold where we just say that everyone is positive or negative, say), so more clever comparison metrics exist. Note also that the ROC AUC does not, in and of itself, say much about out-of-domain generalisation, which might be most important in the real world!\n",
    "\n",
    "## Final words\n",
    "\n",
    "Congratulations. You've implemented regularised logistic regression on a real dataset (that you cleaned up yourself) and made your own ROC curve. We'll now move on to multiclass logistic regression and then to neural networks!\n",
    "\n",
    "## Survey\n",
    "\"I want a Survey, hey! Giving feedback for the very first time. I want a su-u-u-u-rvey, got some feedback, on my mi-i-i-n-d\". Thanks Weird Al, [very cool](https://www.youtube.com/watch?v=notKtAgfwDA). Here you go: [clickety-click](https://docs.google.com/forms/d/e/1FAIpQLSfaeqtRTz5KMqcmxQuOI5GYWHMejjh5_yuiCNSnNblpdKb0hQ/viewform?usp=sf_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
