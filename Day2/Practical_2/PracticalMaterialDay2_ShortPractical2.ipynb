{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 2\n",
    "\n",
    "Welcome to the second practical of today. Here, you will work on implementing regularised logistic regression, as well as implementing cross-validation on some data and making an ROC curve. First run the two cells below to set things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1 / (1 + np.exp(-data))\n",
    "    return output\n",
    "\n",
    "\n",
    "# I have redefined the mySigmoid for numerical stability here.\n",
    "# Why this? Well, with many parameters and large values, numerical precision for the power function becomes\n",
    "# an issue. Read here: https://github.com/scipy/scipy/blob/91a279ecb05e7814e2787bfa618d46ad3e0af2be/scipy/special/_logit.h\n",
    "# how scipy fixes that.\n",
    "def mySigmoid(data):\n",
    "    data = np.array(data)\n",
    "    return expit(data)\n",
    "\n",
    "\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    # make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def linAlgLogRegHypothesis(data, thetas):\n",
    "    output = mySigmoid(linAlgRegHypothesis(data, thetas))\n",
    "    return output\n",
    "\n",
    "\n",
    "def costFuncLogReg(x, y, thetas):\n",
    "    predictions = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    totalCosts = np.nansum(1 / len(x) * costsPerSample)\n",
    "    return totalCosts\n",
    "\n",
    "\n",
    "def makeCrossValData(featureArray, y, k=10):\n",
    "    \"\"\"function to make splits into training and validation sets.\n",
    "    Outputs two lists of length k, where each element is the indices of samples to train on for that fold,\n",
    "    and the indices of samples to test on for that fold, respectively.\"\"\"\n",
    "    m = len(featureArray)\n",
    "    # shuffle data\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    shuffled_features = featureArray[shuffled_indices, :]  # assumes 2D array\n",
    "    shuffled_labels = y[shuffled_indices]\n",
    "    # see how many equal-sized sets you can make\n",
    "    dataPerSplit = int(np.floor(m / k))\n",
    "    dataPartitions = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        # make a list of all the samples for each fold\n",
    "        dataPartitions.append(list(range(counter, counter + dataPerSplit)))\n",
    "        counter += dataPerSplit\n",
    "\n",
    "    samplesEquallySplit = k * dataPerSplit\n",
    "    if not samplesEquallySplit == m:\n",
    "        # after making equal splits there will be samples left, i.e. you cannot always make k exactly evenly sized subsets.\n",
    "        # randomly assign left over samples to folds after\n",
    "        toDivide = m - samplesEquallySplit\n",
    "        for extraSampleIndex in range(counter, counter + toDivide):\n",
    "            # only assign to lists of samples that have the current minimum amount of samples\n",
    "            currentSubsetSizes = np.array([len(subset) for subset in dataPartitions])\n",
    "            assignTo = np.random.choice(\n",
    "                np.where(currentSubsetSizes == np.min(currentSubsetSizes))[0]\n",
    "            )\n",
    "            dataPartitions[assignTo].append(extraSampleIndex)\n",
    "\n",
    "    # Now make the final cross-validation set: make k sets, each set has (k-1)/k folds to train on, and 1 fold to test on.\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for validationSetIndex in range(0, k):\n",
    "        # put 1 fold in the test set\n",
    "        testSet.append(dataPartitions[validationSetIndex])\n",
    "        # put all other folds in the train set\n",
    "        trainSet.append(dataPartitions.copy())\n",
    "        trainSet[validationSetIndex].pop(validationSetIndex)\n",
    "        # this line makes sure all training set indices are in one big list, rather than k-1 small lists.\n",
    "        trainSet[validationSetIndex] = [\n",
    "            item for sublist in trainSet[validationSetIndex] for item in sublist\n",
    "        ]\n",
    "\n",
    "    return shuffled_features, shuffled_labels, trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c72a",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation is a method of automatically constraining how much your model can (over)fit on the training data. We add some factor (regularisation weight $\\lambda$) times the sum of squares of the parameter (excluding the intercept ($\\theta_0$) to the cost function. In this way, the model cannot pick extremely large values for the parameters, i.e. when you have 100 features, the model is forced to only have high $\\theta$ parameters for those features that matter a lot for correct classification, while having extremely low or even 0 values for features that don't. Hence, regularisation also automatically selects features that are of importance to your problem: _feature selection_! (Strictly speaking, this holds only for when you penalise the absolute of the sum of the parameters, not when you penalise the square). Note that once you have trained the model and want to know the cost on the validation/test set, you should not use regularised cost: you care about your performance in the end (which you hope is better because you constrain the parameters during fitting).\n",
    "\n",
    "* To get started, change your costFuncLogReg to have an extra argument `lambda_ = 0` ( _ because lambda is a keword for anonymous functions), that, if set to a value higher than 0, causes regularisation to be performed.\n",
    "* Make sure to exclude the bias/intercept term ($\\theta_0$) from this. By convention this is not regularised.\n",
    "* While you are at it, also reorder the arguments to `thetas, x, y, lambda_=0` so it is easier to use another optimizer if we want to!\n",
    "\n",
    "Hint:\n",
    "* Remember that the regularised logistic regression cost function is:\n",
    "![APicture](RegLogRegEq.PNG) <div>\n",
    "You already had the first part implemented, you only need to add the second part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aaa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a8ee45",
   "metadata": {},
   "source": [
    "## Changing gradient descent\n",
    "\n",
    "The gradients should also change. Luckily, since all that's added is a plus term, the change is extremely minor:\n",
    "![gradients_logreg](GradientsRegLogReg.PNG)\n",
    "\n",
    "* Up to you to implement the changes in the `linAlgGradientDescent` function. Add another `lambda_ = 0` argument and change the gradients as needed. So `linAlgGradientDescent(x, y, thetas, alpha, lambda_ = 0, method = \"linear\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91272388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old function\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, method=\"linear\"):\n",
    "    possible_methods = [\"linear\", \"logistic\"]\n",
    "    if method not in possible_methods:\n",
    "        print(\n",
    "            \"Error! Wrong method given. Should be one of: \"\n",
    "            + str(possible_methods)\n",
    "            + \"\\n Returning None!\"\n",
    "        )\n",
    "        return\n",
    "    m = len(x)\n",
    "    ## all these shape/ndim calls are unnecessary if you input column vectors as you should.\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.shape != (m, 1):\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha / m * gradientSummation\n",
    "    newThetas = thetas - finalGradientSteps.T\n",
    "    return newThetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb46850-3ffa-4614-852d-06379c4379a0",
   "metadata": {},
   "source": [
    "# Refactoring into two separate functions\n",
    "\n",
    "Below, I have made one function called `computeGradients()` that computes and returns the gradients, and another function called `gradientDescentStep()` that takes a step using current thetas, those gradients, and an alpha value. In this way, we can use the first one if we want to use some other optimizer (which wants just the gradients), and the second one if we want to use gradient descent proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d303fd-52a2-49dd-b836-2d8532dd3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor into separate functions\n",
    "def computeGradients(thetas, x, y, lambda_=0, method=\"linear\"):\n",
    "    m = len(x)\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if method == \"linear\":\n",
    "        preds = linAlgRegHypothesis(x, thetas)\n",
    "    else:\n",
    "        preds = linAlgLogRegHypothesis(x, thetas)\n",
    "\n",
    "    if preds.shape != (m, 1):\n",
    "        preds = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    unregularisedGradients = 1 / m * gradientSummation\n",
    "    regularisedGradients = np.ravel(unregularisedGradients)\n",
    "    regularisedGradients[1:] = (\n",
    "        regularisedGradients[1:] + lambda_ / m * np.ravel(thetas)[1:]\n",
    "    )\n",
    "    # print(\"final regularised gradients:\")\n",
    "    # print(regularisedGradients)\n",
    "    return regularisedGradients\n",
    "\n",
    "\n",
    "def gradientDescentStep(thetas, gradients, alpha):\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    if gradients.ndim < 2:\n",
    "        gradients = gradients[:, np.newaxis]\n",
    "    newThetas = thetas - alpha * gradients\n",
    "    return newThetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32fd36",
   "metadata": {},
   "source": [
    "## Loading in some data \n",
    "\n",
    "Let's look at the Pima Indians dataset, which contains information on multiple clinical variables and whether or not patients have diabetes. The below code loads in the data. I am using pandas since it has a nice .describe() method for DataFrame that shows you information about the data. Up to you to investigate this data somewhat:\n",
    "\n",
    "* Are there any NaNs in the data?\n",
    "* Are there other values that seem circumspect? Name 2 examples. How many of these circumspect values are there in these features?\n",
    "* How many cases and controls are there? Is this a balanced dataset?\n",
    "\n",
    "Hint(s):\n",
    "* Use the `.describe` method of the dataframe to help you answer these questions.\n",
    "* You can slice a dataframe using `df.loc[df[\"colName\"] < 12, :]`, which corresponds to getting you only rows for which the values in column _colName_ are less than 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d804a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesData = pd.read_csv(\"PimaIndiansDiabetes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95721312-adbb-4ba6-a016-268d96337cb4",
   "metadata": {},
   "source": [
    "# Crappier data\n",
    "\n",
    "You will have seen that definitely not all samples make sense. Now let's make the problem even worse!\n",
    "\n",
    "To show regularisation in action, it's actually good to have some data that you might overfit on. To do that, I've asked your good friend ChatGPT-4O, and it modified the dataset to include:\n",
    "* Correlated (Redundant) Features: Duplicates of existing features with slight noise.\n",
    "* Irrelevant Noisy Features: Five random noise columns.\n",
    "* Polynomial and Interaction Features: Nonlinear relationships (e.g., Glucose Ã— Insulin).\n",
    "* Weakly Predictive Features: Slightly correlated to the target variable with added noise.\n",
    "* Scaling Inconsistencies: A feature (BMI) scaled by 1000.\n",
    "\n",
    "We'll work with that, for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a424b-d6b9-45d1-acd0-87f60fc0287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_messy = pd.read_csv(\"PimaIndiansDiabetes_MessedUpNoise.csv\")\n",
    "\n",
    "# Ensure the dataset has an 'Outcome' column\n",
    "# Generate 50 'Weak_Signal' columns efficiently\n",
    "if \"Outcome\" in pima_messy.columns:\n",
    "    weak_signals = pima_messy[\"Outcome\"].values[\n",
    "        :, np.newaxis\n",
    "    ] * 0.05 + np.random.uniform(-1, 1, (pima_messy.shape[0], 50))\n",
    "    weak_signal_columns = [f\"Weak_Signal_{i}\" for i in range(1, 51)]\n",
    "\n",
    "    # Generate 200 random noise columns efficiently\n",
    "    random_noise = np.random.uniform(-1, 1, (pima_messy.shape[0], 200))\n",
    "    noise_columns = [f\"Random_Noise_{i}\" for i in range(1, 201)]\n",
    "\n",
    "    # Convert to DataFrames and concatenate efficiently\n",
    "    pima_messy = pd.concat(\n",
    "        [\n",
    "            pima_messy,\n",
    "            pd.DataFrame(weak_signals, columns=weak_signal_columns),\n",
    "            pd.DataFrame(random_noise, columns=noise_columns),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the normalization function\n",
    "def createNormalisedFeatures(featureArray, mode=\"range\", printit=False):\n",
    "    featureMeans = np.mean(featureArray, axis=0, keepdims=True)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis=0, keepdims=True) - np.min(\n",
    "            featureArray, axis=0, keepdims=True\n",
    "        )\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis=0, keepdims=True)\n",
    "        normalisedFeatures = (featureArray - featureMeans) / featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "# Extract the 'Pregnancies' and 'Outcome' columns separately\n",
    "pregnancy_col = pima_messy[\"Pregnancies\"].values[:, np.newaxis]\n",
    "outcome_col = pima_messy[\"Outcome\"].values[:, np.newaxis]\n",
    "# print('Columns in data:\\n')\n",
    "# print('\\n'.join(pima_messy.columns),'\\n')\n",
    "\n",
    "# Replace 0 with NaN for imputation (excluding 'Pregnancies' and 'Outcome')\n",
    "dfSubsetNan_messy = pima_messy.drop(columns=[\"Pregnancies\", \"Outcome\"]).replace(\n",
    "    0, np.nan\n",
    ")\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(missing_values=np.nan)\n",
    "dfSubsetNan_messy = imputer.fit_transform(dfSubsetNan_messy)\n",
    "\n",
    "# Add back the 'Pregnancies' column\n",
    "featuresMessyNotNorm = np.append(dfSubsetNan_messy, pregnancy_col, axis=1)\n",
    "\n",
    "# Normalize using standard deviation method\n",
    "normMessyFeats, meansMessyFeats, standardDevsMessyFeats = createNormalisedFeatures(\n",
    "    featuresMessyNotNorm, \"SD\"\n",
    ")\n",
    "\n",
    "# Store class labels\n",
    "messyClassLabels = outcome_col\n",
    "\n",
    "print(\"final data first 5 rows:\")\n",
    "display(normMessyFeats[:5, :10])\n",
    "display(messyClassLabels[:5])\n",
    "n_parameters_messy = normMessyFeats.shape[1]\n",
    "print(f\"n_parameters_messy: {n_parameters_messy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9f384",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle I\n",
    "\n",
    "Okay, now we can train regularised logistic regression on this data. Let's **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**. We'll downsample the data so we have equal amounts of the positive and negative class, and train the classifier on 80% of the training data while testing on 20% held-out data (normally we'd use cross-validation but let's not put that extra level of complication in here as well). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73704425-db92-4c68-a6b2-34eba4b0173e",
   "metadata": {},
   "source": [
    "Your job:\n",
    "\n",
    "* Downsample the normalised messy Diabetes Data (`normMessyFeats`): remove random rows of the controls so you have equal # of non-diabetes and diabetes cases. You could use `np.random.choice(a=rowIndicesOfoRowsThatDon'tHaveDiabetes, size = howManySamplesNeedToBeRemoved, replace = False)`, where you then remove (`np.delete()` can be useful) those rows from the feature and class label array. You'll probably also need `np.ravel(messyClassLabels)` and `np.where()`. <br> Save the new data as `equalClassSizeDiabetesData` and `equalClassSizeClassLabels` for the labels.\n",
    "Hints: \n",
    "* `np.where` returns a tuple, of which you need the first element.\n",
    "* Note that you can always insert a new cell above or below the current one for testing or debugging using `escape + a` or `escape + b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure everyone gets the same split\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# downsample the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba13b",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle II\n",
    "\n",
    "* Make a list of the lambda values to train on (`lambdaValues`; **use lambda values of 0, 0.5, 1, 5, 10, 100, 1000 and 10000**), an empty list to store the test cost in (`testCostList`), and a list for the final thetas after gradient descent (`finalThetaList`).\n",
    "* Randomly sample 80% of the data you produced above for training, and save the rest for testing. **_Code for this is given below!_**.\n",
    "* Now make a `for`-loop that loops over the different lambdaValues.\n",
    "* In that loop, make another loop that performs 300 gradient descent steps with an alpha of 0.2 on `trainDataDiabetes`.\n",
    "* After that's done, calculate the cost on `testDataDiabetes` **without regularisation (lambda of 0)**. Remember: you don't use the regularisation parameter in the final predictions, because you use it _during training_ to prevent overfitting, and then want to know how well you really do on the test data. \n",
    "* Append the result to the `testCostList`.\n",
    "* Finally, look at the DataFrame containing the theta parameters found for the different values of lambdas, and the cost calculated on the test set (code to make it is given below). What do you see? \n",
    "\n",
    "Hints:\n",
    "* There are many steps here. If you get stuck on one, ask a question or look at the answers to see how to do that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(900)\n",
    "startThetas = np.array([0] * (n_parameters_messy + 1))[:, np.newaxis]\n",
    "nSteps = 500\n",
    "alpha = 0.2\n",
    "\n",
    "# making lists\n",
    "\n",
    "\n",
    "#     code for dividing into 80% and 20%\n",
    "\n",
    "nrSamplesToTake = int(np.ceil(0.8 * np.sum(equalClassSizeClassLabels == 0)))\n",
    "negativeSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 0)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSampleIdxTrain = np.random.choice(\n",
    "    np.arange(0, np.sum(equalClassSizeClassLabels == 1)),\n",
    "    size=nrSamplesToTake,\n",
    "    replace=False,\n",
    ")\n",
    "positiveSamplesTrain, positiveClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTrain, negativeClassLabelsTrain = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTrain, :\n",
    "    ],\n",
    ")\n",
    "trainDataDiabetes = np.vstack([positiveSamplesTrain, negativeSamplesTrain])\n",
    "trainClassLabelsDiabetes = np.vstack(\n",
    "    [positiveClassLabelsTrain, negativeClassLabelsTrain]\n",
    ")\n",
    "\n",
    "\n",
    "negativeSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 0))\n",
    "        if i not in negativeSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSampleIdxTest = np.array(\n",
    "    [\n",
    "        i\n",
    "        for i in np.arange(0, np.sum(equalClassSizeClassLabels == 1))\n",
    "        if i not in positiveSampleIdxTrain\n",
    "    ]\n",
    ")\n",
    "positiveSamplesTest, positiveClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1, :][\n",
    "        positiveSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "negativeSamplesTest, negativeClassLabelsTest = (\n",
    "    equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    "    equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0, :][\n",
    "        negativeSampleIdxTest, :\n",
    "    ],\n",
    ")\n",
    "testDataDiabetes = np.vstack([positiveSamplesTest, negativeSamplesTest])\n",
    "testClassLabelsDiabetes = np.vstack([positiveClassLabelsTest, negativeClassLabelsTest])\n",
    "\n",
    "\n",
    "# your looping code, performing gradient descent for each lambda and\n",
    "# calculating the cost on the test set after it's done, should go here:\n",
    "\n",
    "\n",
    "#     code to make a final DataFrame to show what happens:\n",
    "#     Uncomment all this code at once by selecting it and pressing Ctrl + /\n",
    "\n",
    "# finalThetas = [np.ravel(elem) for elem in finalThetas]\n",
    "# dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.array(testCostList)])\n",
    "# columnNames = [\"theta_\" + str(elem) for elem in list(range(len(startThetas)))]\n",
    "# columnNames.append(\"testSetCost\")\n",
    "# dataFrame.columns = columnNames\n",
    "# dataFrame.set_index(lambdaValues, inplace = True)\n",
    "# display(dataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bbb54",
   "metadata": {},
   "source": [
    "## Regularised logistic regression results\n",
    "\n",
    "If all goes well, you will see that the cost on the test set is lowest when using $\\lambda > 0$. Isn't that grand!? Not too strange when we all but forced overfitting by adding lots of uninformative features.  Spectacularly unconvincing example notwithstanding: in general, for unregularised classification, you run the risk of tuning the parameters _too specifically_ to the values in the training set, which increases the cost on the unseen test set. Regularisation guards against this by penalising too large parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd07cb2",
   "metadata": {},
   "source": [
    "## Classifier performance\n",
    "\n",
    "We've talked in the lectures about the performance of a classification algorithm. We want to know the true positive rate and false positive rates for a given threshold, but also the classifier's performance over a range of thresholds. It is not too difficult to make a ROC curve yourself. Let's do that now for the best classifier (with the lowest mean cost on the test set).\n",
    "\n",
    "Up to you to:\n",
    "* Make a range of 200 thresholds (from 1 to 0) for saying something is the positive set (use `np.linspace` for this).\n",
    "* Make two empty lists: `truePositiveRates` and `trueNegativeRates`.\n",
    "* Make predictions on the `testDataDiabetes` using the best set of learned thetas (which you can manually select or get from `finalThetas` using the index of the minimum element in `testSetCostList`).\n",
    "* Make a for loop over the different thresholds you defined. Within that loop:\n",
    "    * Turn the predictions into class labels using `np.where` and the current threshold value.\n",
    "    * Calculate the true positive rate (sensitivity/recall) and append it to the list.\n",
    "    * Calculate the true negative rate and append it to the `trueNegativeRates` list.\n",
    "* Finally make a plot of the sensitivity (true positive rate) on the y-axis and 1-specificity (1-TNR) on the x-axis. (use `fig, ax = plt.subplots()` and `ax.plot()`). Don't forget to set the axis labels and a title!\n",
    "\n",
    "See the relevant excerpt from the slide below, and look [here](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/) for more explanation if you want it! <br> ![SensitivityAndSpecificity](SensitivityAndSpecificity.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477afa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ed82",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What regularisation is, and how it works (penalising large weights for parameters, thereby forcing the algorithm to focus on those that really give it a lot of _bang for its buck_ and decreasing overfitting)\n",
    "* How to implement regularisation, and how the parameter $\\lambda$ affects it\n",
    "* How to do some basic cleaning on a dataset, and what _the idea_ of imputation is (specifically of a KNNImputer)\n",
    "* How to make a ROC plot, and what exactly is depicted on it, as well as why we might want to compare something like ROC AUC between classifiers, rather than accuracy. Note also that ROC AUC also compares wholly unrealistic thresholds (we would never use a threshold where we just say that everyone is positive or negative, say), so more clever comparison metrics exist. Note also that the ROC AUC does not, in and of itself, say much about out-of-domain generalisation, which might be most important in the real world!\n",
    "\n",
    "## Final words\n",
    "\n",
    "Congratulations. You've implemented regularised logistic regression on a real dataset (that you cleaned up yourself) and made your own ROC curve. We'll now move on to multiclass logistic regression and then to neural networks!\n",
    "\n",
    "## Survey\n",
    "\"I want a Survey, hey! Giving feedback for the very first time. I want a su-u-u-u-rvey, got some feedback, on my mi-i-i-n-d\". Thanks Weird Al, [very cool](https://www.youtube.com/watch?v=notKtAgfwDA). Here you go: [clickety-click](https://docs.google.com/forms/d/e/1FAIpQLSfaeqtRTz5KMqcmxQuOI5GYWHMejjh5_yuiCNSnNblpdKb0hQ/viewform?usp=sf_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
