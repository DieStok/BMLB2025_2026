{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Afternoon practical day 1** <br>\n",
    "Welcome to the afternoon practical. Here, you're going to write a function to use cross-validation rather than training on all data. After that, you'll implement multivariate linear regression with linear algebra using numpy. Finally, you'll run it on a small dataset of SNPs, thereby performing an elementary GWAS analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import random as rand\n",
    "%matplotlib widget\n",
    "import crossValFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary functions you defined before:\n",
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def multiHypothesis(x, thetas, printit = False):\n",
    "    #add a 1 to x as the first 'feature'\n",
    "    one_to_add = np.array([1])\n",
    "    if printit:\n",
    "        print(x)\n",
    "        print(one_to_add)\n",
    "        print(x.shape)\n",
    "        print(one_to_add.shape)\n",
    "    x = np.concatenate((one_to_add, x))\n",
    "    if printit: print(x)\n",
    "    if not len(x) == len(thetas):\n",
    "        print(\"Error, x and theta should have equal length!\")\n",
    "        return\n",
    "    prediction = sum([curr_x * curr_theta for curr_x, curr_theta in zip(x, thetas)])\n",
    "    #faster way\n",
    "    if printit:\n",
    "        print(x) \n",
    "        print(thetas)\n",
    "    prediction = np.sum(x*thetas)\n",
    "    return prediction\n",
    "\n",
    "def MyMSE(x, y, thetas):\n",
    "    totalSumSquares = 0\n",
    "    for index, val in enumerate(x):\n",
    "        prediction = multiHypothesis(val, thetas)\n",
    "        squareError = (prediction-y[index])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(x) \n",
    "    return meanSquaredError\n",
    "\n",
    "def gradientDescent(x, y, thetas, alpha):\n",
    "    m = len(x)\n",
    "    total_error_thetas = np.zeros_like(thetas)\n",
    "    for index, row in enumerate(x):\n",
    "        hypothesis_outcome = multiHypothesis(row, thetas)\n",
    "        row_with_one = np.concatenate((np.array([1]), row))\n",
    "        errors_this_sample = (hypothesis_outcome - y[index]) * row_with_one\n",
    "        total_error_thetas = total_error_thetas + errors_this_sample\n",
    "    final_thetas = thetas - alpha/m * total_error_thetas\n",
    "    return final_thetas\n",
    "\n",
    "def makePolynomialFeatures(x, power = 2, printit = False):\n",
    "    data = []\n",
    "    for i in range (1, power+1):\n",
    "        data.append(x**i)\n",
    "    if printit: print(data)\n",
    "    finalArray = np.vstack(tuple(data)).T\n",
    "    if printit: print(finalArray)\n",
    "    return finalArray\n",
    "\n",
    "def createNormalisedFeatures(featureArray, mode = \"range\", printit = False):\n",
    "    if printit: print(featureArray)\n",
    "    featureMeans = np.mean(featureArray, axis = 0, keepdims=True)\n",
    "    if printit: print(featureMeans)\n",
    "    if printit: print(featureArray-featureMeans)\n",
    "    if mode == \"range\":\n",
    "        featureRanges = np.max(featureArray, axis = 0, keepdims=True) - np.min(featureArray, axis = 0, keepdims=True)\n",
    "        #broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans)/featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = np.std(featureArray, axis = 0, keepdims=True)\n",
    "        #broadcasting in action:\n",
    "        normalisedFeatures = (featureArray - featureMeans)/featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "#sample data for use\n",
    "data = np.loadtxt(\"sampleDataLinearRegression.csv\", delimiter=',')\n",
    "x_data, y_data = data[:,0], data[:,1]\n",
    "\n",
    "featuresHP = makePolynomialFeatures(x_data, power = 2)\n",
    "\n",
    "normalisedFeaturesHP, featureMeansHP, featureRangesHP = createNormalisedFeatures(featuresHP, mode = \"range\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cross-validation\n",
    "So far, we've only been fitting on all the data. That's not great for knowing about how we will generalise, as we have discussed, and we don't know whether we might be overfitting. Here, I show you an example of a cross-validated linear regression. I have supplied a function that does cross-validation (cv), and show you the outcome for a 9-fold cross-validation on the sample linear regression data we have been using all along. At the end of the practical, writing your own cross-validation function (i.e. a function that splits data into k folds and returns those) is an _optional_ extra thing to do. Ten folds are often used (10-fold cross-validation), although that's not because it is universally optimal. See here: https://stats.stackexchange.com/a/357749 and here: https://stats.stackexchange.com/a/264721 for more info. It comes down to the bias-variance trade-off that you have heard about. And maybe to the monkey see monkey do-mindset that is rather more pervasive in science than one might like to admit. That is: everybody uses 10-fold cross-validation so we do it too (to be comparable). <br> For now, let's set that discussion aside, and focus on how cross-validation looks for the case where we fit a quadratic equation to linear data (for some arcane reason). You can scan through the code below, but the gist of it is: it splits the data into 9 folds, fits a linear regression (with two features, one quadratic) on training data, then applies it to the test data, and plots how that looks, along with the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function\n",
    "%matplotlib inline\n",
    "normalised_y_data, _, _ = createNormalisedFeatures(y_data[:, np.newaxis])\n",
    "shuffledFeatureArray, shuffledLabels, trainSetIndices, testSetIndices = (\n",
    "    crossValFunction.makeCrossValData(normalisedFeaturesHP, normalised_y_data, 9)\n",
    ")\n",
    "\n",
    "print(\"List of training set indices to use: \\n\")\n",
    "print(trainSetIndices)\n",
    "print(\"--------\")\n",
    "print(\"List of test set indices to use: \\n\")\n",
    "print(testSetIndices)\n",
    "\n",
    "# make a figure to show this\n",
    "stepsGradDescent = 40\n",
    "# note: this alpha actually results in rising MSE values after a number of gradient descent steps. In a real scenario,\n",
    "# I would recommend an alpha of 0.01 to start\n",
    "alpha = 0.2\n",
    "startThetas = [0, 0, 0]\n",
    "\n",
    "# lists to store information for all cross-validations:\n",
    "thetaValuesDuringDescent = []\n",
    "MSEDuringDescent = []\n",
    "testMSE = []\n",
    "\n",
    "figCrossVal, axsCrossVal = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "\n",
    "\n",
    "for i in range(0, 9):\n",
    "    trainSubset = shuffledFeatureArray[trainSetIndices[i], :]\n",
    "    trainSubsetLabels = shuffledLabels[trainSetIndices[i]]\n",
    "    testSubset = shuffledFeatureArray[testSetIndices[i], :]\n",
    "    testSubsetLabels = shuffledLabels[testSetIndices[i]]\n",
    "    thetaValuesDuringDescent.append([])\n",
    "    MSEDuringDescent.append([])\n",
    "    thetasNow = startThetas.copy()\n",
    "\n",
    "    for steps in range(0, stepsGradDescent):\n",
    "        oneStep = gradientDescent(trainSubset, trainSubsetLabels, thetasNow, alpha)\n",
    "        thetaValuesDuringDescent[-1].append(oneStep)\n",
    "        thetasNow = oneStep\n",
    "        MSEDuringDescent[-1].append(MyMSE(trainSubset, trainSubsetLabels, thetasNow))\n",
    "\n",
    "    # with final thetas, test on the validation set and check MSE\n",
    "    testMSE.append(MyMSE(testSubset, testSubsetLabels, thetasNow))\n",
    "\n",
    "\n",
    "# now plot this data\n",
    "counterPlotting = 0\n",
    "for plotY in range(0, 3):\n",
    "    for plotX in range(0, 3):\n",
    "        # print(counterPlotting)\n",
    "        axsCrossVal[plotX, plotY].scatter(\n",
    "            shuffledFeatureArray[trainSetIndices[counterPlotting], 0],\n",
    "            shuffledLabels[trainSetIndices[counterPlotting], -1],\n",
    "            color=\"blue\",\n",
    "            label=\"training data\",\n",
    "        )\n",
    "        axsCrossVal[plotX, plotY].scatter(\n",
    "            shuffledFeatureArray[testSetIndices[counterPlotting], 0],\n",
    "            shuffledLabels[testSetIndices[counterPlotting], -1],\n",
    "            color=\"red\",\n",
    "            label=\"test data\",\n",
    "        )\n",
    "        # plot predicted test data\n",
    "        axsCrossVal[plotX, plotY].scatter(\n",
    "            shuffledFeatureArray[testSetIndices[counterPlotting], 0],\n",
    "            [\n",
    "                multiHypothesis(x_vals, thetaValuesDuringDescent[counterPlotting][-1])\n",
    "                for x_vals, y_vals in zip(\n",
    "                    shuffledFeatureArray[testSetIndices[counterPlotting], :],\n",
    "                    shuffledLabels[testSetIndices[counterPlotting]],\n",
    "                )\n",
    "            ],\n",
    "            color=\"green\",\n",
    "            label=\"test data predictions\",\n",
    "        )\n",
    "\n",
    "        # plot all data as predicted by these thetas\n",
    "        xValuesToPlotHere = shuffledFeatureArray[trainSetIndices[counterPlotting], 0]\n",
    "        fittedValuesToPlotHere = [\n",
    "            multiHypothesis(x_vals, thetaValuesDuringDescent[counterPlotting][-1])\n",
    "            for x_vals, y_vals in zip(\n",
    "                shuffledFeatureArray[trainSetIndices[counterPlotting], :],\n",
    "                shuffledLabels[trainSetIndices[counterPlotting]],\n",
    "            )\n",
    "        ]\n",
    "        zippedTogether = list(zip(xValuesToPlotHere, fittedValuesToPlotHere))\n",
    "        sortedZipped = sorted(zippedTogether, key=lambda x: x[0])\n",
    "        unzippedXAndFitted = list(zip(*sortedZipped))\n",
    "        xValuesToPlotHere = unzippedXAndFitted[0]\n",
    "        fittedValuesToPlotHere = unzippedXAndFitted[1]\n",
    "        axsCrossVal[plotX, plotY].plot(\n",
    "            xValuesToPlotHere,\n",
    "            fittedValuesToPlotHere,\n",
    "            color=\"black\",\n",
    "            label=\"fitted line\",\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "\n",
    "        axsCrossVal[plotX, plotY].legend(fontsize=\"small\")\n",
    "\n",
    "        # add theta values for this cross-validation\n",
    "        stringToAdd = \"thetas: \" + str(\n",
    "            np.round(thetaValuesDuringDescent[counterPlotting][-1], 2)\n",
    "        )\n",
    "        axsCrossVal[plotX, plotY].text(\n",
    "            0.95,\n",
    "            0.01,\n",
    "            stringToAdd,\n",
    "            verticalalignment=\"bottom\",\n",
    "            horizontalalignment=\"right\",\n",
    "            transform=axsCrossVal[plotX, plotY].transAxes,\n",
    "            color=\"black\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "\n",
    "        # add mean-squared error on the test set\n",
    "        stringToAdd = \"MSE on test data: \" + str(np.round(testMSE[counterPlotting], 2))\n",
    "        axsCrossVal[plotX, plotY].text(\n",
    "            0.95,\n",
    "            0.10,\n",
    "            stringToAdd,\n",
    "            verticalalignment=\"bottom\",\n",
    "            horizontalalignment=\"right\",\n",
    "            transform=axsCrossVal[plotX, plotY].transAxes,\n",
    "            color=\"black\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "        counterPlotting += 1\n",
    "\n",
    "\n",
    "print(\"Average test set error: \" + str(np.round(np.mean(testMSE), 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated polynomial fitting outcome and real-life complications for cross-validation\n",
    "\n",
    "Unsurprisingly, you'll see that the prediction performance is abysmal, and also varies a lot per cross-validation. This is because the polynomial function actually doesn't fit the data as it is linear, and there is extremely little data. Hence, it matters a lot which data points are in the test set and which aren't. The scale makes the MSE seem small, but you'll agree that the fit is not good. <br>\n",
    "Still, it's a good illustration of the power of cross-validation: by seperating our data and training on subsets, we can get a better idea of the generalisation performance, i.e. how well our model will perform on real-world data. If we'd have had a good model rather than a toy one, we could now decide to train the final model on **all** training data, and then publish it and put it to use. <br>\n",
    "\n",
    "There is a catch: cross-validation in k folds assumes that the data we will be predicting on in the real world will be very much _like_ the data we are training the model on. The assumption is called IID, that data is Independent and Identically Distributed. This is often simply not true. The ML field is having a reckoning with this. See for example [this](https://arxiv.org/abs/2210.07242) or [this](https://openaccess.thecvf.com/content/ICCV2021/html/Hendrycks_The_Many_Faces_of_Robustness_A_Critical_Analysis_of_Out-of-Distribution_ICCV_2021_paper.html), or [here, under A. OBSTACLES TO ACHIEVING SAFETY, ROBUSTNESS, AND RELIABILITY](https://ieeexplore.ieee.org/abstract/document/9783196). As an example superhuman classifiers trained on ImageNet fail completely when you show them images from China, because, say, kitchens in China can look vastly different from what has been learned as a kitchen in datasets biased towards European or American images. That might seem an easy solve: just include images from everywhere. Do note: covering every possibility is hard if not impossible. \n",
    "\n",
    "However, consider a hospital setting: you train a classifier on 200 patients' gene expression data to predict whether or not they should have resection of part of their large intestine when they get cancer. This works great, you validate, everything is fine. But shift starts to occur: maybe the gene expression profiling platform changes, the hospital gets a new machine, clinical procedures change, some upstream decision means that the patient population that is screened with your classifier changes, etc. We call this _dataset shift_. It is basically inevitable that you will ask your classifier to work on samples that differ in subtle and not-so-subtle ways from what was seen during training, and that is dangerous. For this reason, more realistic cross-validation procedures are designed, that give a view on generalisation potential under data shift. If you are predicting protein-protein interactions, train on the database containing data up to and until 2018, and then predict on the new data up to 2022. If you are training on gene expression data, leave all expression from a specific study or specific countries or medical centres out, and see how well you generalise on those. There's also ways to monitor whether a sample is very different from all others, but this is a very active field of study.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra exercises\n",
    "\n",
    "Until now, you've been working with for loops and lists, looping over the theta values and corresponding features, multiplying them, then summing these and thereby obtaining the final predictions. For gradient descent, too, we have been working with iteration. As you've been told in the lectures, the language of machine learning is linear algebra. We're now going to focus on implementing what you've done so far in linear algebra. Before we move on, if you're still a little bit unsure about some of what we've done until now, please watch some of the videos given in the course reader, for example on linear regression, the bias-variance trade-off, or cross-validation.\n",
    "\n",
    "Let's get a bit of a feel for working with matrices. To do that, we'll do some pen-and-paper exercises:\n",
    "* Go [here](https://www.algebrapracticeproblems.com/matrix-multiplication-product-multiplying-matrices/), read the text and do the first 3 practice problems. You can check your solutions on the site. Be sure to read through the procedures and properties of matrix multiplication so you're sure about your understanding.\n",
    "* After that's done, do the 3 problems below: <br> <br> <br>\n",
    "\n",
    "1. $\\begin{bmatrix} 6 & -2 & 5 \\\\ 1 & 6 & 2 \\\\ -3 & 4 & 7 \\end{bmatrix} \\cdot \\begin{bmatrix} -4 \\\\ 8 \\\\ 3 \\end{bmatrix}$ <br> <br>\n",
    "2. $\\begin{bmatrix} 2 & -6 & 11 \\end{bmatrix} \\cdot \\begin{bmatrix} -16 \\\\ -5 \\\\ 2 \\end{bmatrix}$ <br> <br>\n",
    "3. $\\begin{bmatrix} 2 & -6 & 11 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ -6 \\\\ 11 \\end{bmatrix}$\n",
    "\n",
    "<br> <br> <br> <br> \n",
    "\n",
    "\n",
    "Note that problem 3 is the same as summing the element-wise squares of the vector entries. In other words: squaring vector elements and summing them is equal to $V^t*V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking your answers with numpy\n",
    "\n",
    "Instead of supplying answers for the above, how's about you check it yourself using numpy? Use the following commands:\n",
    "\n",
    "* `np.array([[6, 12, 3], [4, 5, 6]])` will result in $\\begin{bmatrix} 6 & 12 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
    "* `A = np.array([[6, 12, 3], [4, 5, 6]])` <br>`B = np.array([[3, 4], [-2, 8], [8, 9]])` <br> `A @ B` <br> \n",
    "will result in $\\begin{bmatrix} 6 & 12 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 4 \\\\ -2 & 8 \\\\ 8 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 147 \\\\ 50 & 110 \\end{bmatrix}$ <br>\n",
    "* `A.transpose()` or `A.T` will yield $\\begin{bmatrix} 6 & 4 \\\\ 12 & 5 \\\\ 3 & 6 \\end{bmatrix}$ (useful for the 3rd problem so you don't repeat yourself)\n",
    "\n",
    "Check your answers for the three final questions above using these commands in the code cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting the univariate and multivariate regression function with linear algebra\n",
    "\n",
    "In the code cell below, you've been given working examples of the regression hypothesis functions you've been working with. Your job is to make just one function `linAlgRegHypothesis(data, thetas)` that can perform linear regression for any number of features, using linear algebra . To do this, remember the following:\n",
    "* You can just prepend 1 to a feature vector to make the multiplication of a theta-vector with those features incredibly straightforward.\n",
    "* You don't need to loop: you can just calculate all predicted values at once using matrix-vector multiplication.\n",
    "* Use `np.array(normalisedFeaturesHP.loc[:, [\"xtoPower1\", \"xtoPower2\"]])` for testing. You'll probably have to use the transpose function.\n",
    "* To add a column of ones to a feature matrix, use `np.c_[arrayOfOnes, featureArrayOrMatrix]` They should of course have the same length (number of rows).\n",
    "\n",
    "If you have trouble grappling with numpy, read [this](https://numpy.org/doc/stable/user/absolute_beginners.html) and use [the numpy documentation](https://numpy.org/doc/stable/reference/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def multiHypothesis(x, thetas, printit = False):\n",
    "    #add a 1 to x as the first 'feature'\n",
    "    one_to_add = np.array([1])\n",
    "    if printit:\n",
    "        print(x)\n",
    "        print(one_to_add)\n",
    "    x = np.concatenate((one_to_add, x))\n",
    "    if not len(x) == len(thetas):\n",
    "        print(\"Error, x and theta should have equal length!\")\n",
    "        return\n",
    "    prediction = sum([curr_x * curr_theta for curr_x, curr_theta in zip(x, thetas)])\n",
    "    #faster way\n",
    "    prediction = np.sum(x*thetas)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "#your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two functions for a given theta vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case something got overwritten\n",
    "data = np.loadtxt(\"sampleDataLinearRegression.csv\", delimiter=',')\n",
    "x_data, y_data = data[:,0], data[:,1]\n",
    "featuresHP = makePolynomialFeatures(x_data, power = 2)\n",
    "normalisedFeaturesHP, featureMeansHP, featureRangesHP = createNormalisedFeatures(featuresHP, mode = \"range\")\n",
    "\n",
    "thetas        = np.array([12, 3, 5])\n",
    "#data          = np.array(normalisedFeaturesHP.loc[:, [\"xtoPower1\", \"xtoPower2\"]])\n",
    "oldFuncOutput = np.array([multiHypothesis(row, thetas) for row in normalisedFeaturesHP])\n",
    "newFuncOutput = linAlgRegHypothesis(normalisedFeaturesHP, thetas)\n",
    "\n",
    "print(\"Old output: \\n\")\n",
    "print(str(oldFuncOutput) + \"\\n\")\n",
    "print(\"New output: \\n\")\n",
    "print(str(newFuncOutput))\n",
    "\n",
    "print(np.isclose(oldFuncOutput, newFuncOutput))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a mean-squared error function and gradient descent with linear algebra\n",
    "\n",
    "Now that you've performed this feat for the hypothesis function, let's do the same for the myMSE function and gradient descent. Start with the Mean-Squared error function. \n",
    "\n",
    "So:\n",
    "\n",
    "* Rewrite the `myMSE` function (call it `linAlgMSE`) to use linear algebra to immediately calculate the MSE for all samples given the current thetas (rather than looping over them). Also make it use the new `linAlgRegHypothesis` function you defined above!\n",
    "* Test that it works using the data provided in the cell below (normalisedFeaturesHP, y_data, and thetas). The answer MSE should be ~327.  \n",
    "\n",
    "Hints:\n",
    "\n",
    "* Make sure that the shape of the predictions made with `linAlgRegHypothesis` and of `y` are the same when subtracting. i.e. make them both row-vectors or column-vectors. To make the predictions a column vector, use `np.reshape(predictions, (len(predictions), 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to use for testing\n",
    "print(normalisedFeaturesHP[0:3,:])\n",
    "print(y_data[0:3])\n",
    "thetas = np.array([50, -48, 155])\n",
    "\n",
    "\n",
    "#old function\n",
    "def MyMSE(x, y, thetas):\n",
    "    totalSumSquares = 0\n",
    "    for index, val in enumerate(x):\n",
    "        prediction = multiHypothesis(val, thetas)\n",
    "        squareError = (prediction-y[index])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(x) \n",
    "    return meanSquaredError\n",
    "\n",
    "#your answer here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with old myMSE\n",
    "Let's compare your outcomes with the old way of doing things. Run the cell below to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldMSEResults = MyMSE(normalisedFeaturesHP, y_data, thetas)\n",
    "newMSEResults = linAlgMSE(normalisedFeaturesHP, y_data, thetas)\n",
    "print(\"Old function result: \")\n",
    "print(str(oldMSEResults) + \"\\n\")\n",
    "print(\"New function result: \")\n",
    "print(newMSEResults)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to gradient descent\n",
    "\n",
    "Now do the same thing for gradient descent. Below I give you the old gradient descent function and some Numpy arrays to reiterate how they work. Call the function you make  `linAlgGradientDescent`. It should have arguments `(x, y, thetas, alpha)`. You can use [this link](https://medium.com/analytics-vidhya/vectorized-implementation-of-gradient-descent-in-linear-regression-12a10ea37210) if you want to get a bit more insight. The basic things to realise are:\n",
    "\n",
    "* If you add 1 as the first feature (as in calculation of the predictions), then the partial derivative for the intercept will automatically just be multiplied by 1.\n",
    "* You can calculate the predictions with the new `linAlgRegHypothesis` you made.\n",
    "* Watch out that your arrays are the same shape and in the correct column form. To get a column array from a normal (1D) numpy array, use `oneDArray[:, np.newaxis]`. To check array shapes, use `.shape`. If used on a 1D array like `np.array([15, 20, 30]).shape`, it will return `(3,)`. If used on a 2D column vector array like `np.array([[1],[5],[18]]).shape` it will return `(3,1)`. See the example below. You can also use the `.ndim` attribute. If it is 1, it is a 1D array, if 2 a 2D array, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, thetas, alpha):\n",
    "    m = len(x)\n",
    "    total_error_thetas = np.zeros_like(thetas)\n",
    "    for index, row in enumerate(x):\n",
    "        hypothesis_outcome = multiHypothesis(row, thetas)\n",
    "        row_with_one = np.concatenate((np.array([1]), row))\n",
    "        errors_this_sample = (hypothesis_outcome - y[index]) * row_with_one\n",
    "        total_error_thetas = total_error_thetas + errors_this_sample\n",
    "    final_thetas = thetas - alpha/m * total_error_thetas\n",
    "    return final_thetas\n",
    "\n",
    "#numpy example shape\n",
    "aOneDArray = np.array([15, 20, 30])\n",
    "print(\"1D Array: \\n\")\n",
    "print(aOneDArray)\n",
    "print(\"Its shape attribute: \\n\")\n",
    "print(aOneDArray.shape)\n",
    "print(\"---------\")\n",
    "print(\"2D Array (column vector) \\n\")\n",
    "aTwoDArray = np.array([[1],[5],[18]])\n",
    "print(aTwoDArray)\n",
    "print(\"Its shape attribute: \\n\")\n",
    "print(aTwoDArray.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "#subtracting these leads to a matrix where the first row contains [15,20,30] - 1, the second one [15,20,30] - 5, etc.:\n",
    "print(\"Subtracting 2D array from 1D array results in a matrix: \\n\")\n",
    "print(aOneDArray - aTwoDArray)\n",
    "print(\"\\n\")\n",
    "#make a 1D array into a 2D array with 1 column and len(1DArray) rows:\n",
    "print(\"You can transform the 1D array into a 2D one using reshape: \\n\")\n",
    "columnVectorOneDArray = np.reshape(aOneDArray, (len(aOneDArray), 1))\n",
    "print(columnVectorOneDArray)\n",
    "print(columnVectorOneDArray.shape)\n",
    "\n",
    "print(\"You can also transform a 1D array into a 2D one using np.newaxis: \\n\")\n",
    "print(aOneDArray[:, np.newaxis])\n",
    "\n",
    "#now you can subtract the two column vectors\n",
    "print(\"Now you get a simple subtraction to get a new vector: \\n\")\n",
    "print(columnVectorOneDArray - aTwoDArray)\n",
    "print(\"Number of dimensions: \" + str((columnVectorOneDArray - aTwoDArray).ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make your new function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your new gradient descent function\n",
    "Now let's test your new function. Let's use just the x1 feature (i.e. search for optimal parameters for univariate linear regression on the test data set). Note the use of `np.newaxis` to make sure that selecting just one column of the feature array doesn't result in a 1D array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "singleFeatData = normalisedFeaturesHP[:,0][:,np.newaxis]\n",
    "print(f'Features (range-normalised): {singleFeatData}\\n')\n",
    "trueValues     = y_data[:,np.newaxis]\n",
    "trueValues     = (trueValues - np.mean(trueValues))/np.std(trueValues)\n",
    "print(f'True values (range-normalised): {trueValues}\\n')\n",
    "stepsToTake    = 30\n",
    "alpha          = 0.9\n",
    "startThetas    = np.array([[1],[0]])\n",
    "\n",
    "\n",
    "#Performing gradient descent\n",
    "thetasDuringDescent       = [startThetas.copy()]\n",
    "JsGradientDescent         = [linAlgMSE(singleFeatData, trueValues, np.ravel(thetasDuringDescent[-1].T))]\n",
    "for step in range(0, stepsToTake):\n",
    "    currentThetas = thetasDuringDescent[-1]\n",
    "    newThetas     = linAlgGradientDescent(singleFeatData, trueValues, currentThetas, alpha)\n",
    "    newJ          = linAlgMSE(singleFeatData, trueValues, newThetas)\n",
    "    \n",
    "    thetasDuringDescent.append(newThetas)\n",
    "    JsGradientDescent.append(newJ)\n",
    "\n",
    "\n",
    "##Plotting shenanigans##\n",
    "\n",
    "#set up 2 plots\n",
    "figGradDescent, axGradDescent = plt.subplots(nrows = 1, ncols = 2, figsize=(9,5.5))\n",
    "#figGradDescent.tight_layout()\n",
    "#colors for plotting gradient descent steps\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "\n",
    "#scatterplot without normalised x values\n",
    "axGradDescent[0].scatter(singleFeatData, trueValues, marker='x', s=40, color='k')\n",
    "axGradDescent[0].set_ylabel(\"y\")\n",
    "axGradDescent[0].set_xlabel(\"x\")\n",
    "\n",
    "#calculate cost values for many different theta0, theta1 combinations\n",
    "theta0Vals =  np.linspace(-5, 5, 200)\n",
    "theta1Vals =  np.linspace(-5, 5, 200)\n",
    "X, Y = np.meshgrid(theta0Vals, theta1Vals)\n",
    "JVals = np.zeros(np.shape(X))\n",
    "for i in range(0,len(theta0Vals)):\n",
    "    for j in range(0,len(theta1Vals)):\n",
    "        JVals[i,j] = linAlgMSE(singleFeatData, trueValues, np.array([X[i,j], Y[i,j]]))\n",
    "\n",
    "#we can reuse the contour plot values we calculated before\n",
    "contours = axGradDescent[1].contour(X, Y, np.round(JVals,1),\n",
    "                         levels = 10)\n",
    "axGradDescent[1].clabel(contours)\n",
    "axGradDescent[1].set_xlabel(r\"$\\theta_0$\")\n",
    "axGradDescent[1].set_ylabel(r\"$\\theta_1$\")\n",
    "minimum = axGradDescent[1].scatter(X[JVals == np.min(JVals)], Y[JVals == np.min(JVals)],\n",
    "                      color = \"blue\", label = \"minimum\", marker = \"x\",\n",
    "                      s = 30)\n",
    "\n",
    "#make the plot of gradient descent steps:\n",
    "\n",
    "#if stepsToTake > colors, repeat the colors\n",
    "colors = colors * int(np.ceil(stepsToTake/len(colors)))\n",
    "\n",
    "#plot initial regression line\n",
    "axGradDescent[0].plot(singleFeatData, linAlgRegHypothesis(singleFeatData, startThetas),\n",
    "           color=colors[0], lw=2, linestyle = \"dashed\",\n",
    "                        label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(startThetas[0,0], startThetas[1,0])) \n",
    "\n",
    "#plot regression lines for each update\n",
    "for j in range(1,stepsToTake):\n",
    "    axGradDescent[1].annotate('', xy=np.ravel(thetasDuringDescent[j]), xytext=np.ravel(thetasDuringDescent[j-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    axGradDescent[0].plot(singleFeatData, linAlgRegHypothesis(singleFeatData, thetasDuringDescent[j]),\n",
    "               color=colors[j], lw=1.5, alpha = 0.6, linestyle = \"dashed\",\n",
    "               label=list(map(r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format,*thetasDuringDescent[j])))\n",
    "\n",
    "    \n",
    "#add points in contour plot for different updated thetas\n",
    "pointColors = colors.copy()\n",
    "pointColors.insert(0, \"black\")\n",
    "axGradDescent[1].scatter(*zip(*thetasDuringDescent), c=pointColors[0:len(thetasDuringDescent)], s=40, lw=0)\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "axGradDescent[1].set_xlabel(r'$\\theta_0$')\n",
    "axGradDescent[1].set_ylabel(r'$\\theta_1$')\n",
    "axGradDescent[1].set_title('Cost function')\n",
    "\n",
    "axGradDescent[0].set_title('Data and fit')\n",
    "axbox = axGradDescent[0].get_position()\n",
    "# Position the legend by hand so that it doesn't cover up any of the lines.\n",
    "#if not stepsToTake > 10:\n",
    "#    axGradDescent[0].legend(fontsize='small')\n",
    "\n",
    "figGradDescent.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I want you to remember here\n",
    "* You need to understand the logic behind k-fold cross-validation and why we do it: by subdividing our data into training folds and test folds, we have a number of estimates for how well our classifier will perform on unseen data. In other words: how well it will _generalise_. You don't, in the end, _use_ any of the classifiers you train during cross-validation: your final classifier would be trained on all your training data, and then published or shared with people. This means that your cross-validation estimates are even slightly pessimistic: more training data usually increases performance, and with cross-validation you always train on slightly less data than you have!\n",
    "* How matrix-vector and matrix-matrix multiplication works, i.e. how to do it and how to code it with numpy. You should be able to perform some simple linear algebra multiplications by hand.\n",
    "* How we use these types of multiplication to streamline and (especially for large datasets) speed up our implementations of ML algorithms, and specifically how you can implement linear regression and gradient descent using it. (Note: I don't expect you to know the exact implementations by heart, but I do want you to know the gist of the multiplications you're doing).\n",
    "* That with these humble beginnings, you can already implement (a facsimile of) a GWAS procedure: relating a continuous variable to the nr. of alternative alleles you have at certain SNP locations (this you can try your hand at in the optional part below, or you just believe me). Pretty neat!\n",
    "\n",
    "## The end\n",
    "\n",
    "Congratulations, you've mastered linear regression using gradient descent and linear algebra.\n",
    "\n",
    "## Survey\n",
    "Did you like this practical? Did you hate it with a burning passion? Fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLSeJXmTlxMXUyCAndmuWVG4FhdFzNm4DEsy1nwswP7SAhOQwoA/viewform?usp=sf_link) and let us know (yes I became an us just now)!\n",
    "\n",
    "## Start of the optional part\n",
    "\n",
    "An earlier course installment made it clear that I was _ever so slightly_ too optimistic about what is achievable in one day while you are being hit with an onslaught of Numpy programming,partial derivatives and linear algebra left and right. So here are optional materials. This means you don't have to do them. Just in case you missed it: you can skip the below optional materials. \n",
    "\n",
    "However, what you _cannot_ skip is filling out the survey on what you thought of this practical. It's right up there ^"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Making a function that splits your training data into k folds for cross-validation\n",
    "\n",
    "Now it's up to you to make a function `makeCrossValData(x, y, k=10)` that can take our data and create k folds from it for cross-validation.\n",
    "\n",
    "So, what we want it to use as input and give as output: \n",
    "* It takes in an array with features and with labels.\n",
    "* It should return four things total:\n",
    "1. It should return two lists of length k. One list (trainSet) contains the row indices for the training data for a certain fold, the other (testSet) contains the row indices of the test data for a certain fold. This means that these lists are lists of lists: `trainSet[0]` should return something like `[3, 9, 14, 15, 20, 34]`, which are all the indices of the training samples to train on for the first cross-validation fold.\n",
    "2. It should return the shuffled x and y arrays. <br> <br>\n",
    "So, in sum, the return statement should look like `return shuffled_features, shuffled_labels, train_indices, test_indices`, where train_indices and test_indices are lists of length k.\n",
    "\n",
    "What we want it to do:\n",
    "* It first shuffles the data. You want to shuffle both the features and the labels/true values in the same way. You can use `np.random.permutation()` for this.\n",
    "* It then makes k splits of the data. Note: you probably can't split the data exactly equally. Hence, use something like `int(np.floor(m/k))` , where m = # of samples, to determine the size of the equal splits you can make. And/Or use the modulo operator (`%`) to see the remainder after division by k (i.e. `9 % 4` = 1, because 2\\*4 = 8, and then 1 is left).\n",
    "* Make sure to identify the samples that remain because they can't be divided equally among the folds, and assign them to random folds.\n",
    "\n",
    "Hints:\n",
    "* This is somewhat difficult, requiring a bit of looping and checking to make sure that you first make proper-sized folds, then adding the remaining samples to random folds, and then outputting a correct list.\n",
    "* If you need it: `[item for sublist in listWithSublists for item in sublist]` will take a list that looks like this: `[[25, 12, 3], [18, 33, 21], [1, 13, 5]]` and turn it into `[25, 12, 3, 18, 33, 21, 1, 13, 5]`. This is called a [list comprehension](https://www.datacamp.com/community/tutorials/python-list-comprehension) and is very powerful (but can also be somewhat unclear, and remember that one of the main purposes of code should be to be intelligible to humans that have to read and/or maintain it!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your answer\n",
    "def makeCrossValData(featureArray, y, k=10):\n",
    "    '''function to make splits into training and validation sets.\n",
    "    Outputs two lists of length k, where each element is the indices of samples to train on for that fold, \n",
    "    and the indices of samples to test on for that fold, respectively.'''\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: moving on to a GWAS(-like) analysis\n",
    "\n",
    "Hopefully, it's a success! If not, be sure to check the shape of your arrays with `.shape` and `.ndim`, and try to figure out what inputs are going awry. <br> <br> <br> Now let's move on to some (very limited) GWAS analysis, based on data from [here](https://github.com/MareesAT/GWA_tutorial/). Below, we load in some example data and explore it. Before this can be done, do the following:\n",
    "\n",
    "* Open an Anaconda prompt, activate the environment you use for the course (if using), and type `conda install -c conda-forge pandas-plink`\n",
    "* You should have done this already in the pre-course setup but it's understandable if you missed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_plink import read_plink1_bin\n",
    "euroGWASData = read_plink1_bin(\"HapMap_3_r3_1.bed\", \"HapMap_3_r3_1.bim\", \"HapMap_3_r3_1.fam\", verbose=True)\n",
    "print(euroGWASData)\n",
    "print(euroGWASData.sel(sample = \"NA06989\"))\n",
    "print(euroGWASData.sel(sample = \"NA06989\").values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the GWAS dataset\n",
    "The print-out of this data is rather complicated. That's because it is a read-in of binary datafiles in PLINK format, a command-line programme that is often used for GWAS analysis. The data is saved in an xarray, which is a specialised datatype that we won't go into here. Note that the rows have samples, and the columns contain features (here: 0, 1 or 2 for a certain SNP). Do the following:\n",
    "\n",
    "* Go [here](https://pandas-plink.readthedocs.io/en/latest/usage.html) and read the entry under Genotype (ignore the Kinship matrix part). After that, read [this](https://pandas-plink.readthedocs.io/en/latest/api/pandas_plink.read_plink1_bin.html)\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "1. How many SNPs are in this data?\n",
    "2. How many SNPs are on chromosome 11?\n",
    "3. How many male genomes are in this dataset? (see [this](https://www.cog-genomics.org/plink/1.9/formats#fam))\n",
    "4. How many genomes lack gender annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP analysis on chromosome eleven subset.\n",
    "Let's continue with only SNPs on chromosome eleven, and 'only' 200 SNPs there. That's still plenty of data. As you've been told, a GWAS simply trains a linear model on the #of alles you have for a certain SNP: if A is normal at a location and you have 2* A, you have 0. If you have a T in 1 copy of that gene, you have 1. If you are homozygously different from A (2 Ts), you have a 2. In contrast to more complex algorithms, SNPs are supposed to have additive effects: we simply tally the effect of one SNP on having the outcome measure with all others, and the total would be the expected effect of your genetic make-up on some outcome like BMI. Within SNPs, too, we generally assume an additive model: that having 2 alternative alleles at a SNP location has twice the effect of having 1. In reality, of course, there can be epistasis, or one allele could do nothing but having 2 mutations has a very large effect (All this is simplifying the field of GWAS, of course, there's a _lot_ of nuance we're leaving out for the purpose of brevity).\n",
    "\n",
    "The cell immediately below makes sure that there is a simulated quantitative outcome measure (BMI), and that a few SNPs on chromosome eleven influence this trait. You can just run it and move on to the next cell to run linear regressions on those SNPs using your own functions. Below are the well-known Manhattan plots of GWAS, which require p-values for the associations in the linear regressions. We haven't talked about how to get those.\n",
    "![Manhattan plots](ManhattanPlot.jpg)\n",
    "\n",
    "It's outside of the scope here to go all-in on explaining this, but you can calculate the standard error of a regression slope. The formula for it is: $$\\sqrt{\\frac{1}{m-2} \\cdot \\frac{\\sum_{i=1}^m(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^m(x_i - x_{mean})^2}}$$\n",
    "Here, $m$ is the number of samples, the top calculates sum of square errors, and the bottom calculates the sum of squares of the features (since we mean-center our features the mean should be 0). The upshot is that this formula calculates how much, on average, the real values differ from the predicted values. For further info, see [this StatQuest video](https://www.youtube.com/watch?v=XNgt7F6FqDU) and [this web page](https://www.statology.org/standard-error-regression/).\n",
    "\n",
    "With this formula in hand, we can calculate a t-statistic, and with a t-statistic, we can perform significance testing, i.e. we can say what the chance is that a result this strong or stronger could have occured just by chance (Although of course significance testing and/or hypothesis testing has received a lot of flak, and rightly so, see: [1](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx), [2](https://arbital.com/p/likelihoods_not_pvalues/), and [3](https://sci-hub.se/https://doi.org/10.17763/haer.48.3.t490261645281841). For a zealous account, see [4](https://sci-hub.se/https://doi.org/10.5153%2Fsro.3857)). Still, that discussion is not the focus here. Calculating the t-statistic is extremely simple: it's just $t = \\frac{\\theta_1}{SE(\\theta_1)}$\n",
    "\n",
    "Intuitively, you can understand that the right plot below has lower standard error, so we are more sure of our estimated slope of the regression line than we are on the left, where it feels like the slope might be very different if we had more samples. We can codify this with a p-value, as is done in GWAS (although see the above _strong condemnations_ of p-values).\n",
    "![plotSERegression](ExampleSERegression.PNG)\n",
    "\n",
    "I supply functions to calculate the p-value from the linear regressions you fit, then we'll look at the 20 lowest p-values.\n",
    "\n",
    "That was a lot of text. Summary:\n",
    "* Run the cell below. It results in data that has 200 SNP positions on chromosome 11, of which 20 have an effect on BMI.\n",
    "* Move on to the cell below that to get to work doing the linear regressions you need to do\n",
    "* Use the supplied functions to calculate the p-value for the slope of each regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsizedData = euroGWASData.where(euroGWASData.chrom == \"11\", drop = True)\n",
    "downsizedData = downsizedData[:, rand.sample(range(0, downsizedData.shape[1]), 200)]\n",
    "#remove columns that have all 0, 1 or 2 --> can't infer anything there\n",
    "columnsWithoutVariation = np.all(downsizedData.values == downsizedData.values[0,:], axis = 0)\n",
    "downsizedData = downsizedData[:, np.invert(columnsWithoutVariation)]\n",
    "\n",
    "\n",
    "#sample some SNPs to assign a signal\n",
    "SNPsForSignal = np.array(rand.sample(range(0, downsizedData.shape[1]), 20))\n",
    "\n",
    "#add normally distributed BMI value with mean that is somewhat healthy \n",
    "newArray = np.zeros(len(downsizedData.trait))\n",
    "downsizedDataNew = downsizedData.drop_vars(\"trait\")\n",
    "newArray = np.random.default_rng().normal(22.5, 1, len(newArray))\n",
    "downsizedDataNew[\"BMI\"] = (\"sample\", newArray)\n",
    "\n",
    "\n",
    "#now, give certain SNPs a protective effect for BMI (help in being leaner) and a few the effect of predisposing to high BMI\n",
    "#assume additivity within the SNP, which is to say: one protective allele has half the protective effect of 2.\n",
    "protectiveSNPs = np.array(rand.sample(list(SNPsForSignal), 5))\n",
    "diseaseSNPs    = np.array([i for i in SNPsForSignal if i not in protectiveSNPs])\n",
    "\n",
    "protectiveEffect = 3\n",
    "for protSNPCoord in protectiveSNPs:\n",
    "    twoAlleles  = np.where(downsizedDataNew[:, protSNPCoord].values == 2.)\n",
    "    oneAllele   = np.where(downsizedDataNew[:, protSNPCoord].values == 1.)\n",
    "    noMuts      = np.where(downsizedDataNew[:, protSNPCoord].values == 0.)\n",
    "    newArray[twoAlleles] -= np.random.default_rng().normal(protectiveEffect, 0.1, len(twoAlleles))\n",
    "    newArray[oneAllele]  -= np.random.default_rng().normal(protectiveEffect/2, 0.1, len(oneAllele))\n",
    "\n",
    "obesityEffect = 3.87\n",
    "for disSNPCoord in diseaseSNPs:\n",
    "    twoAlleles  = np.where(downsizedDataNew[:, disSNPCoord].values == 2.)\n",
    "    oneAllele   = np.where(downsizedDataNew[:, disSNPCoord].values == 1.)\n",
    "    noMuts      = np.where(downsizedDataNew[:, disSNPCoord].values == 0.)\n",
    "    newArray[twoAlleles] += np.random.default_rng().normal(obesityEffect, 0.1, len(twoAlleles))\n",
    "    newArray[oneAllele]  += np.random.default_rng().normal(obesityEffect/2, 0.1, len(oneAllele))\n",
    "\n",
    "\n",
    "downsizedData[\"BMI\"] = (\"sample\", newArray)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing your analysis\n",
    "Okay, the data is all set up. There is just one thing: there are missing data in this GWAS dataset: SNPs that were not measured for certain people. For now, we'll just replace these NaNs with 0, assuming they are the most common genotype. In fact, there's two things: the second one is that there are SNPs where every person in the dataset has either 0, 1, or 2: we can't say anything there. I've prefiltered those columns out for you in the code cell above. Now to run the linear regressions as used in GWAS:\n",
    "* Loop over the columns of the `downsizedData`. Each column contains data for a single SNP. To get at this data, use `downsizedData[:, columnNr].values`\n",
    "* This SNP data is your feature. Run linear regression on this feature, using the BMI as the outcome. Don't forget to add the intercept feature which is just an array of ones (or alternatively, this might be incorporated into your linAlgGradientDescent function already).\n",
    "* To run linear regression:\n",
    "    * Make sure you normalise the feature and the BMI.\n",
    "    * Initialise thetas (done for you).\n",
    "    * Run gradient descent for a number of steps (nSteps = 20 below)\n",
    "    * Save the resulting theta values in a list or array, so that you get, per SNP in the data, the two thetas.\n",
    "\n",
    "After this is done, we'll worry about calculating the p-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up. Set mean BMI as start value for intercept.\n",
    "startThetas = np.array([[np.mean(newArray)], [0]])\n",
    "nSteps      = 20\n",
    "outcome     = downsizedData[\"BMI\"].values\n",
    "#Normalise the outcome measure!\n",
    "\n",
    "alpha       = 0.3 \n",
    "listThetasForEachSNP = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#up to you: use gradient descent to find the thetas, save these thetas in a list after all nSteps are done.\n",
    "#don't forget to normalise BMI and the features!\n",
    "for SNPIndex in range(0, downsizedData.shape[1]):\n",
    "    #replace np.nan with 0\n",
    "    values = downsizedData[:, SNPIndex].values\n",
    "    values[np.isnan(values)] = 0\n",
    "    \n",
    "    for step in range(0, nSteps):\n",
    "        break\n",
    "    break\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the p-values for each regression\n",
    "\n",
    "Run the cell below to get the function you need. Note: you separately fit a regression for each SNP, and this function works on the theta parameters (here, only $\\theta_1$) for each of these regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPValuesRegSlope(X, y, thetas):\n",
    "    \"\"\"Skips calculation for the significance of the intercept (i.e. it being sig. different from 0,\n",
    "    as it's the relation between BMI and the SNP we are interested in. Assumes theta as a vector (2D numpy array) of\n",
    "    n_thetas by 1 (so 2 rows, 1 column for univariate linear regression)\"\"\"\n",
    "    m = len(X)\n",
    "    thetasToCalc = thetas[1:,:]\n",
    "    preds = linAlgRegHypothesis(X, thetas)\n",
    "    #print(preds)\n",
    "    #print(y)\n",
    "    if not y.ndim > 1:\n",
    "        y = y[:, np.newaxis]\n",
    "    #print(X)\n",
    "\n",
    "    sumSquareErrorsPred = np.sum(np.square(np.array(preds) - y))\n",
    "    stdErrors = []; tStats = []; pVals = []\n",
    "    for index, theta in enumerate(thetasToCalc):\n",
    "        #print(theta) ;print(index)\n",
    "        sumSquaresFeature = np.sum(np.square(X - np.mean(X)))\n",
    "        stdError = np.sqrt((1/(m-2)) * (sumSquareErrorsPred/sumSquaresFeature))\n",
    "        tStat    = theta/stdError\n",
    "        pVal     = scipy.stats.t.sf(abs(tStat), m-1)\n",
    "        stdErrors.append(stdError); tStats.append(tStat); pVals.append(pVal)\n",
    "    \n",
    "    df = pd.DataFrame(np.vstack([stdErrors, tStats, pVals]))\n",
    "    df.set_index(pd.Index([\"standard errors\", \"t statistics\", \"p-values\"]), inplace = True)\n",
    "    return[pVals, df]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the p-values\n",
    "\n",
    "The below code will get you p-values for every SNP. All you need to do is give it a list of thetas to work on. That list of thetas should have n_SNPs entries, and each entry is, in this case, a 2 by 1 2D array, containing $\\theta_0$ and $\\theta_1$ for the linear regression of BMI against # of alternative alleles for that SNP. Up to you to:\n",
    "\n",
    "* **Replace listThetasForEachSNP in `enumerate(listThetasForEachSNP)` with your name for a list with the 2 thetas for each of the 200 linear regressions you trained**\n",
    "* Inspect the p-values. How many are less than 0.05? How many would you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pValsPerSNP = []\n",
    "listDFs = []\n",
    "outcomeNorm, outcomeMean, outcomeSD = createNormalisedFeatures(outcome, mode = \"SD\")\n",
    "dataWithoutNA = downsizedData.values; dataWithoutNA[np.isnan(dataWithoutNA)] = 0\n",
    "dataNorm, dataMeans, dataSDs        = createNormalisedFeatures(pd.DataFrame(dataWithoutNA), mode = \"SD\")\n",
    "for SNPindex, thetas in enumerate(listThetasForEachSNP):\n",
    "    pVal, dfInfo = calcPValuesRegSlope(np.array(dataNorm.iloc[:, SNPindex]), outcomeNorm, thetas)\n",
    "    pValsPerSNP.append(pVal[0])\n",
    "    listDFs.append(dfInfo)\n",
    "\n",
    "pValuesPerSNPFinal = np.ravel(np.hstack(pValsPerSNP))\n",
    "print(pValuesPerSNPFinal)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the regression values you obtained\n",
    "\n",
    "Now, let's take the 20 lowest p-values values and see whether these correspond to the signal we put into the BMI values. Remember, I added signal to 20 SNPs so you would find it easily in this _very small_ practice dataset. Of course, in normal GWAS procedure they use very stringent genome-wide p-value thresholds, much lower than 0.05, or they do some other sort of multiple testing correction, and often the SNPs are validated later. For our simple purposes here, though, let's just take the 20 lowest p-values, i.e. the SNPs where the chance that this regression slope or a more extreme one was found by simple chance/noise is lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowestPValues = np.argsort(pValuesPerSNPFinal)[0:20]\n",
    "# check whether this is the signal I put in\n",
    "matches  = np.in1d(SNPsForSignal, lowestPValues)\n",
    "nMatches = sum(matches)\n",
    "print(\"SNPs with top 20 lowest p-values which are indeed of the 20 to which I added signal: \" + str(nMatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "If things are similar on your side, you'll get that about half (I had 12/20) were indeed the SNPs where I put signal in. This shows that correction for spurious hits is a big deal in GWAS: even though I _added_ a lot of over-the-top signal, we still didn't retrieve all of them. Of course, our sample size is very low, which makes spurious results easier. Below we plot the regressions you fitted for the SNPs that had actual business being found as significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the linear regressions for SNPs with large effect sizes\n",
    "\n",
    "The below plots the linear regression plots for those SNPs that have actual signal. **Again, replace listThetasForEachSNP with the correct name for your list of theta parameters for each linear regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "%matplotlib inline \n",
    "\n",
    "SNPsWithLowPValues = SNPsForSignal[matches]\n",
    "\n",
    "outcomeNorm, outcomeMean, outcomeSD = createNormalisedFeatures(outcome, mode = \"SD\")\n",
    "\n",
    "neededData = downsizedData[:, SNPsWithLowPValues]\n",
    "\n",
    "for SNPindex, SNPlocation in enumerate(SNPsWithLowPValues):\n",
    "    thetasHere = listThetasForEachSNP[SNPlocation]\n",
    "    #twentyLargestThetaOnes.iloc[0:2, np.where(twentyLargestThetaOnes.columns == SNPlocation)[0][0]]\n",
    "    #maybe return to normal coordinates\n",
    "    features = neededData.values[:, SNPindex]\n",
    "    features[np.isnan(features)] = 0\n",
    "    featuresNorm, mean, SD = createNormalisedFeatures(features, mode= \"SD\")\n",
    "    SNPsPresentInData = np.in1d(np.array([0., 1.,2.]), features)\n",
    "    #skip SNPs where not all 3 genotypes are present, complicates plotting considerably\n",
    "    if not np.all(SNPsPresentInData):\n",
    "        continue\n",
    "    normalAllele = neededData[\"a0\"].values[SNPindex]\n",
    "    alternativeAllele = neededData[\"a1\"].values[SNPindex]\n",
    "    xLabels = [normalAllele*2, normalAllele+alternativeAllele, alternativeAllele*2]\n",
    "    xLabels = np.array(xLabels)[np.where(np.in1d(np.array([0., 1.,2.]), features))]\n",
    "    SNPName = neededData[\"snp\"].values[SNPindex]\n",
    "    #only predict once for each possible feature value\n",
    "    uniqueAlleleNr = np.sort(np.unique(featuresNorm))\n",
    "    predictions = linAlgRegHypothesis(uniqueAlleleNr, thetasHere)\n",
    "    #print(predictions)\n",
    "    #print(predictions * SD + mean)\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    ax.scatter(featuresNorm, outcomeNorm *outcomeSD + outcomeMean, edgecolors = \"black\")\n",
    "    ax.set_xticks(np.unique(featuresNorm))\n",
    "    ax.set_xlabel(\"SNP genotype\")\n",
    "    ax.set_xticklabels(xLabels.tolist())\n",
    "    ax.set_ylabel (\"BMI\")\n",
    "    ax.set_title(SNPName + \"; p-value (uncorrected for multiple testing): \" + str(np.round(pValuesPerSNPFinal[SNPlocation], 4)) + \"; $\\\\theta_1$: \" + str(np.round(np.ravel(thetasHere)[1], 3)))\n",
    "    ax.plot(uniqueAlleleNr, predictions * outcomeSD + outcomeMean, color = \"red\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are here. Did you bring your towel?\n",
    "\n",
    "Darn, what a rush. I hadn't expected anyone after all this optionality. Well, congrats. If you're reading this you're either a) capable of scrolling all the way down in a Jupyter Notebook (no easy feat for sure), or b) went ahead and made your own cross-validation function and did a faux-GWAS analysis. Wow! \n",
    "\n",
    "I'd like to stress once more that the 'GWAS' we've done here is a parody of the real thing. There's familial relations to take into account (you share many SNPs with parents and grandparent), sex assignment to check, missing data to impute or otherwise correct for, etc. Nevertheless, the principle of running a linear regression for each SNP is the same. At the end of the week, we'll be able to correct a GWAS for population covariance in SNPs which will already bring us closer to the real deal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
